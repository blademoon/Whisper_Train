{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b1b3eb-dfbd-41cb-9656-26f992163aae",
   "metadata": {},
   "source": [
    "# Предварительная настройка среды разработки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7cfc2-ab19-4ebd-b39a-ceb906c5bbf3",
   "metadata": {},
   "source": [
    "## Глобальные настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05b20d-ecce-4884-8c70-da71d00cb32d",
   "metadata": {},
   "source": [
    "Определеим некоторые настройки работы для упрощения управления процессом обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a6b180-bf66-4eca-aa1d-ec87d9df53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./DATA/\" # Относительный путь к папке для хранения данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0f01-dd3a-4bfb-9037-c54e8a0a0184",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Google Collab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45f27e-f772-4937-b7dd-ba6837555684",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Подключение сервисов Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e5335-b715-4cf6-9681-c954c58f28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e26633-ae35-4b63-818c-d2ead300d88e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798cb81e-2303-4cba-bc69-30d3d059ccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b24bcf-fda3-4c4b-86ae-5979b36ef554",
   "metadata": {},
   "source": [
    "## Локальная среда"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8e550-e5d6-4476-96c9-a37e30016c33",
   "metadata": {},
   "source": [
    "### Проверка доступности GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed895ca-e0e7-478d-9020-f3cc7b7f937f",
   "metadata": {},
   "source": [
    "Чтобы получить GPU, нажмите _Runtime_ -> _Change runtime type_, затем измените _Hardware accelerator_ с _None_ на _GPU_.\n",
    "\n",
    "Мы можем проверить, что нам назначен GPU, и просмотреть его характеристики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8217a839-6751-4ae2-a1a4-9544a4496546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 12 12:50:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 527.92.01    Driver Version: 528.02       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   35C    P8     6W /  75W |      9MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A        32      G   /Xwayland                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc17294-0ccd-4844-a52d-5b09b0a5f4e3",
   "metadata": {},
   "source": [
    "### Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e881c0-5258-47d4-8119-16c237bc8a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: evaluate in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: multiprocess in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2.7.1)\n",
      "Requirement already satisfied: packaging in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (22.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.0.0)\n",
      "Requirement already satisfied: pandas in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (1.5.2)\n",
      "Requirement already satisfied: dill in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: jiwer in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from levenshtein==0.20.2->jiwer) (2.13.7)\n",
      "Requirement already satisfied: gradio in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (3.16.1)\n",
      "Requirement already satisfied: uvicorn in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (4.4.0)\n",
      "Requirement already satisfied: pandas in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.5.2)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (10.4)\n",
      "Requirement already satisfied: jinja2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: httpx in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.23.3)\n",
      "Requirement already satisfied: markupsafe in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: ffmpy in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: fastapi in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.89.1)\n",
      "Requirement already satisfied: pillow in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (9.3.0)\n",
      "Requirement already satisfied: orjson in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.8.5)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: markdown-it-py[linkify,plugins] in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.28.1)\n",
      "Requirement already satisfied: numpy in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: fsspec in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2022.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.6.2)\n",
      "Requirement already satisfied: pycryptodome in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.16.0)\n",
      "Requirement already satisfied: aiohttp in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.8.3)\n",
      "Requirement already satisfied: pydantic in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.10.4)\n",
      "Requirement already satisfied: python-multipart in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: pydub in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: entrypoints in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->gradio) (2022.7)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: starlette==0.22.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from starlette==0.22.0->fastapi->gradio) (3.6.2)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (0.16.3)\n",
      "Requirement already satisfied: certifi in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (2022.12.7)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
      "Requirement already satisfied: mdit-py-plugins in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.3)\n",
      "Requirement already satisfied: linkify-it-py~=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from python-multipart->gradio) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests->gradio) (1.26.13)\n",
      "Requirement already satisfied: click>=7.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from uvicorn->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from uvicorn->gradio) (0.14.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
      "Library installation complete!\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge transformers\n",
    "!conda install -y -c conda-forge datasets\n",
    "!conda install -y -c conda-forge librosa\n",
    "!conda install -c conda-forge ffmpeg\n",
    "!conda install -c conda-forge huggingface_hub\n",
    "\n",
    "!pip install --no-input evaluate\n",
    "!pip install --no-input jiwer\n",
    "!pip install --no-input gradio\n",
    "\n",
    "print(\"Library installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756687d-d234-4d16-990a-8d2000ef2652",
   "metadata": {},
   "source": [
    "### Загрузка необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e6056b6-30b5-4d17-9f8d-2158e12b57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries/modules initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# Вскпомогательные бибилиотеки\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from datasets import Audio\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "\n",
    "# Evaluate\n",
    "import evaluate\n",
    "\n",
    "print(\"Required libraries/modules initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecc953-8821-4064-97cc-dc17b5ccc74c",
   "metadata": {},
   "source": [
    "### Настройка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5f54433-bfb3-48f9-9ce7-a980d4c0a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of workers processes is set at 16\n",
      "Environment setup completed!\n"
     ]
    }
   ],
   "source": [
    "# Определим количество ядер CPU, это влияет на количество рабочих процессов которые мы можем запустить одновременно\n",
    "system_num_workers = multiprocessing.cpu_count()\n",
    "print(\"The number of workers processes is set at\", system_num_workers)\n",
    "\n",
    "# # Аутентификация ноутбука в HuggingFace HUB\n",
    "# notebook_login()\n",
    "\n",
    "print(\"Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147ebd1-58e2-4ad7-a397-24e6805bd1a0",
   "metadata": {},
   "source": [
    "# Тонкая настройка Whisper для многоязыкового ASR с помощью библиотеки 🤗 Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca114a35-8e37-4e5d-bd57-165a43b8f79c",
   "metadata": {},
   "source": [
    "В этом блокноте мы представляем пошаговое руководство по тонкой настройке Whisper для любого многоязыкового набора данных ASR с помощью библиотеки Hugging Face 🤗 Transformers. Это более \"практическая\" версия сопутствующего [сообщения в блоге](https://huggingface.co/blog/fine-tune-whisper). Для более подробного объяснения Whisper, набора данных Common Voice и теории, лежащей в основе тонкой настройки, читателю рекомендуется обратиться к статье в блоге."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b0dcf-f121-407d-8688-ae7eed7bff9c",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5234d-999c-4ea6-aef6-a6a8f2fae199",
   "metadata": {},
   "source": [
    "Whisper - это предварительно обученная модель для автоматического распознавания речи (ASR) опубликованная в [сентябре 2022](https://openai.com/blog/whisper/) авторами Алеком Рэдфордом и другими из OpenAI. В отличие от многих своих предшественников, таких как. \n",
    "[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), которые предварительно обученны на неразмеченных аудиоданных, Whisper предварительно обучен на огромном количестве **размеченных** транскрибированных аудио данных, 680 000 часов, если быть точным. Это на порядок больше данных, чем использованные для обучения Wav2Vec 2.0 (60 000 часов) неразмеченных аудиоданных. Более того, 117 000 часов из этих данных использованных для предварительного обучения - это многоязыковые данные ASR. Это позволяет получить контрольные точки которые могут быть применены к более чем 96 языкам, многие из которых считаются _низкоресурсными_.\n",
    "\n",
    "При масштабировании на 680 000 часов размеченных данных предварительного обучения, модели Whisper демонстрируют высокую способность к обобщению для многих наборов данных и областей (доменов). Предварительно обученные контрольные точки достигают результатов, конкурентоспособных с современными \n",
    "ASR системами, с коэффициентом ошибок в словах (WER) около 3% на подмножестве чистых тестов LibriSpeech ASR и новым передовым результатом на TED-LIUM с 4,7% (смотрите таблицу 8 из [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)). \n",
    "\n",
    "Обширные знания о многоязычном ASR, полученные Whisper во время предварительного обучения могут быть использованы для других языков с низким уровнем ресурсов; посредством тонкой настройки предварительно обученные контрольные точки могут быть адаптированы для конкретных наборов данных и языков для дальнейшего улучшения результатов. Мы покажем, как можно точно настроить Whisper для языков с ограниченными ресурсами в этом блокноте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b4cc4-1ec3-4857-b11f-8aff83c0e06f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/whisper_architecture.svg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Рисунок 1:</b> модель Whisper. Архитектура соответствует стандартной модели кодера-декодера на основе трансформатора. Log-Mel спектрограмма подается на вход кодера. Последние скрытые состояния поступают в декодер через механизмы перекрестного внимания. Декодер авторегрессионно предсказывает текстовые токены, совместно обусловленные скрытыми состояниями энкодера и ранее предсказанными токенами. Источник рисунка: \n",
    "<a href=\"https://openai.com/blog/whisper/\">блог OpenAI Whisper</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731f8d5-ebd9-4a90-9564-a1b91ea5d890",
   "metadata": {},
   "source": [
    "Контрольные точки Whisper представлены в пяти конфигурациях с различными размерами моделей. Самые маленькие четыре модели обучаются либо только на английском, либо на многоязычных данных. Самая большая контрольная точка - только многоязычная. \n",
    "Все девять предварительно обученных контрольных точек доступны на сайте [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). Контрольные точки сведены в следующую таблицу со ссылками на модели в хабе:\n",
    "\n",
    "| Размер   | Количество слоёв | Ширина | Количество голов | Параметры | Только англоязычная модель                                         | Многоязычная модель                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "В демонстрационных целях мы доработаем контрольную точку многоязычной версии [``малой модели``](https://huggingface.co/openai/whisper-small) с 244M параметрами (~= 1GB). Что касается наших данных, мы обучим и оценим нашу систему на языке с низким уровнем ресурсов, взятом из набора данных [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Мы покажем, что всего за 8 часов тонкой настройки данных мы можем добиться высоких результатов на этом языке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d7e44-a8a4-4640-a07e-6ecb1b9f5bd0",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "${}^1$ - Название Whisper происходит от аббревиатуры \"WSPSR\", которая расшифровывается как \"Web-scale Supervised Pre-training for Speech Recognition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da24b6-3178-4538-8cd5-57ff8b22aaba",
   "metadata": {},
   "source": [
    "## Загрузка набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90357e66-1041-4c4f-9e7e-d40d94cae25b",
   "metadata": {},
   "source": [
    "Используя 🤗 Datasets, загрузка и подготовка данных чрезвычайно проста. \n",
    "\n",
    "Мы можем загрузить и подготовить сплиты Common Voice всего одной строкой кода. \n",
    "\n",
    "Во-первых, убедитесь, что вы приняли условия использования на Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). После принятия условий вы получите полный доступ к набору данных и сможете загружать данные локально.\n",
    "\n",
    "Объединим `train` и `validation` части набора данных, чтобы получить как можно больше данных для обучения. Будем использовать\n",
    "данных `test` в качестве нашего тестового набора.\n",
    "\n",
    "> ВАЖНО!\n",
    "> Для удобства хранения данных датасета, я ранее определил путь к папке в переменной `DATA_PATH`. Это позволит хранить кеш данных в известном заранее месте и в дальшнейшем удалить ненужные данные. Это позволит быстро освободить место на HDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa87700-7e44-457c-a593-38b9747263f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './DATA/' already exist!\n"
     ]
    }
   ],
   "source": [
    "# Создадим папку для хранения данных локально\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "    print(\"Directory '% s' created\" % DATA_PATH)\n",
    "else:\n",
    "    print(\"Directory '% s' already exist!\" % DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a82d17d-69bc-4e13-bdf1-4a058c367733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset common_voice_2_0/ru to /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1de6df724641a696f35e85c6425e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/588M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset common_voice_2_0 downloaded and prepared to /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_2_0 (/home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 3898\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 1561\n",
      "    })\n",
      "})\n",
      "Data download successfully completed!\n"
     ]
    }
   ],
   "source": [
    "# Создадим словарь для хранения датасета\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_2_0\", \"ru\", split=\"train+validation\", cache_dir=DATA_PATH, use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_2_0\", \"ru\", split=\"test\", cache_dir=DATA_PATH, use_auth_token=True)\n",
    "\n",
    "print(common_voice)\n",
    "print(\"Data download successfully completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a920f22-3ede-492d-a7eb-9e764d3241c1",
   "metadata": {},
   "source": [
    "Большинство наборов данных ASR предоставляют только входные образцы аудио (`audio`) и соответствующий транскрибированный текст (`sentence`).  Common Voice содержит дополнительную метаданные, такие как `accent` и `locale`, которые мы можем игнорировать для ASR.\n",
    "\n",
    "Оставив блокнот максимально общим, мы рассматриваем только входной звук и транскрибированный текст для тонкой настройки, отбрасывая дополнительную информацию метаданных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d79484dc-05a9-42b6-aed9-5289b0c60a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 3898\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 1561\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b572fa-b08d-4e94-8d20-6c341f23d1da",
   "metadata": {},
   "source": [
    "## Подготовка экстрактора признаков, токенизатора и данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e3a16-2c58-4e1c-808c-985adb82f22e",
   "metadata": {},
   "source": [
    "Конвейер ASR можно разделить на три этапа: \n",
    "1) экстрактор признаков, который предварительно обрабатывает необработанные аудиоданные\n",
    "2) Модель, которая выполняет сопоставление последовательности с последовательностью \n",
    "3) Токенизатор, который постобрабатывает выводы модели в текстовый формат.\n",
    "\n",
    "Модель Whisper из библиотеки 🤗 Transformers имеет ассоциированный экстрактор признаков и токенизатор, называемые [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
    "и [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer) \n",
    "соответственно.\n",
    "\n",
    "Рассмотрим детальнее настройки экстрактора и токенизатора по очереди!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e8c78-2986-4597-850d-5cbfcfdcdf78",
   "metadata": {},
   "source": [
    "### Загрузка WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da804fb-d0f5-49ae-8b4b-c370f135286a",
   "metadata": {},
   "source": [
    "Экстрактор признаков Whisper выполняет две операции:\n",
    "1. Разбивка / усечение аудиоданных до 30 секунд: все аудиоданные короче 30 секунд разбиваются на молчание (нули), а те, что длиннее 30 секунд, усекаются до 30 секунд.\n",
    "2. Преобразует входные аудио в _log-Mel спектрограммы_ входных признаков, визуальное представление аудио и форму входных данных, ожидаемых моделью Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c57886-db51-4867-86d8-33495e18d5b3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Рисунок 2:</b> Преобразование дискретизированного аудио массива в log-Mel спектрограмму.\n",
    "Слева: дискретизированный одномерный аудиосигнал. Справа: соответствующая log-Mel спектрограмма. Источник рисунка:\n",
    "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eabd2e-88c6-4617-8296-21f26e3dfc8e",
   "metadata": {},
   "source": [
    "Мы загрузим экстрактор признаков из предварительно обученной контрольной точки со параметрами по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a4b5d8-b946-48d0-b349-85db08a1968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature extractor is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "print(\"The feature extractor is loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf53f7-bec1-44ca-b1db-1aa395cef30f",
   "metadata": {},
   "source": [
    "### Загрузка WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d921f8-b3a4-4afc-8d67-35281063e47e",
   "metadata": {},
   "source": [
    "Модель Whisper выдает последовательность идентификаторов _токенов_. Токенизатор сопоставляет каждый из этих идентификаторов токенов с соответствующей текстовой строкой. Для русского языка мы можем загрузить предварительно обученный токенизатор и использовать его для тонкой настройки \n",
    "с указанием целевого языка и задачи. Эти аргументы сообщают токенизатору о том, что следует задавать префикс токена языка и задачи в начале последовательности меток:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a3ad868-efde-495e-82aa-780f5c4b9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Russian\", task=\"transcribe\")\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434f3bf-5240-4650-9ca7-57edec2a59dd",
   "metadata": {},
   "source": [
    "### Комбинирование для создания WhisperProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d28df3-e0de-499d-8141-991d1785282f",
   "metadata": {},
   "source": [
    "Чтобы упростить использование экстрактора признаков и токенизатора, мы можем _обернуть_ их в один класс `WhisperProcessor`. Этот объект наследуется от `WhisperFeatureExtractor` и `WhisperProcessor`, и может использоваться для обработки входных аудиоданных и \n",
    "получения предсказаний модели по мере необходимости. При этом во время обучения нам нужно отслеживать только два объекта: `processor` и `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ef4a58f-393e-4a6f-8813-fa3d7904b961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Russian\", task=\"transcribe\")\n",
    "\n",
    "print(\"Processor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a495cd5-cd68-4cb5-934b-4a0d885c38c1",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfe667-ebeb-43cf-8bed-14a666eb7dec",
   "metadata": {},
   "source": [
    "Давайте распечатаем первый образец набора данных Common Voice, чтобы посмотреть в какой форме находятся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80eb74a1-99c2-4165-a151-f03a9b2b044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': 'f247238b250c437f020fb0bdd95984cc8d3686ce4b07d952a6d541dc3e25a7a512797cc0432ebd4a917ca83890bd9e7aa4aa732154092a09f4cef429df32a87c', 'path': 'DATA/downloads/extracted/acaf2c2e872a1554fdde5f2e9f14ba8c2f8793da2206b75dafc9b15a35bc343a/clips/001b11fa8d4b0277e41ab82671346ec1b8851e6ed7162c12952a45c9ffc3d8a61b333e92188ebfc6d61b128e1967fb0074b4d22208dc32f41a4f4a23e6ec1f00.mp3', 'audio': {'path': 'DATA/downloads/extracted/acaf2c2e872a1554fdde5f2e9f14ba8c2f8793da2206b75dafc9b15a35bc343a/clips/001b11fa8d4b0277e41ab82671346ec1b8851e6ed7162c12952a45c9ffc3d8a61b333e92188ebfc6d61b128e1967fb0074b4d22208dc32f41a4f4a23e6ec1f00.mp3', 'array': array([ 0.0000000e+00,  4.5755869e-13,  8.2663303e-13, ...,\n",
      "       -1.3604904e-07, -2.0340853e-05, -7.7015411e-06], dtype=float32), 'sampling_rate': 48000}, 'sentence': 'Да ты уже просто с катушек съехал!', 'up_votes': 2, 'down_votes': 0, 'age': 'fourties', 'gender': 'male', 'accent': '', 'locale': '', 'segment': ''}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250c661-248c-48f0-bb0c-0097d6455c73",
   "metadata": {},
   "source": [
    "Поскольку наш входной звук сэмплирован с частотой 48 кГц (`'...sampling_rate': 48000...`), нам нужно _уменьшить_ его до частоту дискретизации до 16 кГц перед тем, как передать его в экстрактор признаков Whisper, 16 кГц - это частота дискретизации, ожидаемая моделью Whisper. Мы установим для аудио входов правильную частоту дискретизации с помощью метода [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column). Эта операция не изменяет звук \"на месте\", а скорее дает сигнализирует `datasets` передискретизировать аудиосэмплы _на лету_ при при первой загрузке данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d59a5789-c125-4371-96c5-81f7d345774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample rate set successfully.\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(\"Audio sample rate set successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691d753-3d67-4724-bc08-d8018ba66795",
   "metadata": {},
   "source": [
    "Повторная загрузка первого аудиообразца из набора данных Common Voice приведет к его передискретизации до нужной частоты дискретизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e51d92-dd89-4690-ae1d-aa9cc358163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': 'f247238b250c437f020fb0bdd95984cc8d3686ce4b07d952a6d541dc3e25a7a512797cc0432ebd4a917ca83890bd9e7aa4aa732154092a09f4cef429df32a87c', 'path': 'DATA/downloads/extracted/acaf2c2e872a1554fdde5f2e9f14ba8c2f8793da2206b75dafc9b15a35bc343a/clips/001b11fa8d4b0277e41ab82671346ec1b8851e6ed7162c12952a45c9ffc3d8a61b333e92188ebfc6d61b128e1967fb0074b4d22208dc32f41a4f4a23e6ec1f00.mp3', 'audio': {'path': 'DATA/downloads/extracted/acaf2c2e872a1554fdde5f2e9f14ba8c2f8793da2206b75dafc9b15a35bc343a/clips/001b11fa8d4b0277e41ab82671346ec1b8851e6ed7162c12952a45c9ffc3d8a61b333e92188ebfc6d61b128e1967fb0074b4d22208dc32f41a4f4a23e6ec1f00.mp3', 'array': array([ 3.0140032e-13,  7.2592689e-15, -1.1101574e-12, ...,\n",
      "       -1.6450653e-05,  2.4153996e-07, -1.7190577e-06], dtype=float32), 'sampling_rate': 16000}, 'sentence': 'Да ты уже просто с катушек съехал!', 'up_votes': 2, 'down_votes': 0, 'age': 'fourties', 'gender': 'male', 'accent': '', 'locale': '', 'segment': ''}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36b2eb-851f-4f6f-9222-18d24a18de87",
   "metadata": {},
   "source": [
    "Теперь мы можем написать функцию для подготовки наших данных к работе с моделью:\n",
    "1. Мы загружаем и передискретизируем аудиоданные, вызывая `batch[\"audio\"]`. Как объяснялось выше, 🤗 Datasets выполняет все необходимые операции по передискретизации на лету.\n",
    "2. Мы используем экстрактор признаков для вычисления входных признаков спектрограммы log-Mel из нашего одномерного аудио массива.\n",
    "3. Мы кодируем транскрипции в идентификаторы меток с помощью токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f51abe62-7257-4363-8e15-6abfbe65cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Загрузим и передискретизировать аудиоданные с 48 до 16 кГц\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # вычислим log-Mel входные признаки из входного аудио массива  \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # закодируем целевой текст в идентификаторы меток\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0f12d-1dc0-4b09-9a0e-36883e91231e",
   "metadata": {},
   "source": [
    "Мы можем применить функцию подготовки данных ко всем нашим учебным примерам, используя метод `.map` в наборе данных. Аргумент `num_proc` указывает, сколько ядер процессора использовать. Если задать `num_proc` > 1, то будет включена многопроцессорная обработка. Если метод `.map` зависает при многопроцессорной обработке, установите `num_proc=1` и обрабатывайте набор данных последовательно.\n",
    "\n",
    "> ВАЖНО!\n",
    "> Для удобства работы я использую автоматическое определения количества рабочих прощессов через переменную `system_num_workers`. Это позволяет существенно повысить производительность и сэкономить время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "911c267f-d85d-4e47-8f40-549ce904ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number of CPU cores is 16.\n",
      "                   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac9c5f1c08c402abf92aca06e03ee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edbf32f65e84815bf30a822d99a969e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd16cf567f44391b0b5b8a749855b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be3e8383bac4b7594901412b7d91b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1940a9a4fd7c4815bbeea44c51e2440a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a83a826a65a4e00b24bec9a7bad9004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa3fd63450e439aa47cebd782a2690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ad4a119d3a46d2aa20854f853f272b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbd8d5323924fadbe96d8e778673fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/243 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a677ec182c4c77a7f6d5579ae95194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#13:   0%|          | 0/243 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e9bd09d5e84564b9b4673209cafe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#12:   0%|          | 0/243 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51960be2746343d9b45e382ce6ff2d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/243 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82056a3f78d248c5a90613c42cabc8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#14:   0%|          | 0/243 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f88b937794485b8768d53749413c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ce45c9b4cb44b9ba6da665042bc2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/244 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4288f525ea471d83486ce50b0ee9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#15:   0%|          | 0/243 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c6331cbc0449ce847b04ebeb3ea9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42134b411e8744a19202c8783c9aef75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ed49717080435aad806cfdab4919a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ae981d62784a04b8ee8761a8a7828e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf1e5a0d5a84efc8a1c7f9904f86670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617b5cd3eb3749c6a32e818129ee1265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e530027c850409f88e2cd0fcc47d164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0888a83e8f4b5fa1985722c3fcef50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3022528aa04103ae97f834075f7acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f251f3cc8b29443c81224d9c86dfd5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdabeaea928948f7bc0031869984fc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/98 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb075bfd02d4c72be3c8ee1bfc1e1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6342a9d4cf294880bc1cb3aa929b422c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#12:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66255c4b904d4227903be856962d4f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#15:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767890bc18d4c7d8d33592a0e577590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#14:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b528bcac6104721b0979c06e4048f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#13:   0%|          | 0/97 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The current number of CPU cores is {}.\".format(system_num_workers))\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=system_num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b781aaa-1077-42d8-9a77-5d634db60f88",
   "metadata": {},
   "source": [
    "## Обучение и оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1beb657-2884-480c-9a1b-5529d039b6c1",
   "metadata": {},
   "source": [
    "Теперь, когда мы подготовили наши данные, мы готовы погрузиться в конвейер обучения. [🤗Тренер (Trainer)](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) сделает за нас большую часть тяжелой работы. Все, что нам нужно сделать, это:\n",
    "- Определить коллатор данных (data collator): коллатор данных берет наши предварительно обработанные данные и подготавливает тензоры PyTorch, готовые для модели.\n",
    "- Выбрать метрику для оценки: во время оценки мы хотим оценить модель с помощью метрики [word error rate (WER)](https://huggingface.co/metrics/wer). Нам нужно определить функцию `compute_metrics`, которая будет обрабатывать эти вычисления.\n",
    "- Загрузить предварительно обученную контрольную точку: нам нужно загрузить предварительно обученную контрольную точку и правильно настроить ее для обучения.\n",
    "- Определить конфигурацию процесса обучения: она будет использоваться 🤗 тренером для определения расписания тренировок.\n",
    "\n",
    "После того как мы произведем тонкую настройку модели, мы оценим ее на тестовых данных, чтобы убедиться, что мы правильно обучили ее транскрибировать речь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5882122-798a-4cd2-b0f8-ec5c290c49ea",
   "metadata": {},
   "source": [
    "### Определение коллатора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78853c4-bae6-46c9-8536-ae7bc8663f5f",
   "metadata": {},
   "source": [
    "Коллатор данных для модели последовательной речи уникален в том смысле, что он обрабатывает входные признаки `input_features` и метки `labels` независимо друг от друга: входные признаки `input_features` должны быть обработанны экстрактором признаков, а метки `labels` - токенизатором. \n",
    "\n",
    "Входные признаки `input_features` уже разбиты на аудиофрагменты длительностью 30 секунд и преобразованы в соответствующие им спектрограммы log-Mel фиксированной размерности с помощью экстрактора признаков, поэтому все, что нам нужно сделать, это преобразовать входные признаки `input_features` в тензоры PyTorch. Мы делаем это с помощью метода `.pad` экстрактора признаков с хаданием параметра `return_tensors=pt`.\n",
    "\n",
    "С другой стороны, `метки (labels)` не имеют вставок (un-padded). Сначала мы выравниваем последовательности до максимальной длины в батче, используя метод `.pad` токенизатора. Затем токены-вставки заменяются на значение `-100`, чтобы эти токены **не** учитывались при вычислении потерь. Затем мы вырезаем токен BOS из начала последовательности меток, так как мы добавим его позже во время обучения. \n",
    "\n",
    "Мы можем использовать препроцессор `WhisperProcessor`, который мы определили ранее, для выполнения извлечения признаков, так и для работы с токенами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d9a116b-8b60-451b-b274-f177c6adcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c9591-cda5-47fb-8438-7d0d1859fc08",
   "metadata": {},
   "source": [
    "Давайте инициализируем коллатор данных, который мы только что определили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "682bd554-b814-457e-8239-2d9db732c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f1ac2-419e-4e34-b6c8-96dee3ede7a8",
   "metadata": {},
   "source": [
    "### Метрика оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689a5a1-64ba-4fe9-aac4-f2c7859f2ed7",
   "metadata": {},
   "source": [
    "Мы будем использовать метрику коэффициента ошибок в словах (WER), которая является \"де-факто\" метрикой для оценки систем ASR-систем. За дополнительной информацией обратитесь к документу WER [docs](https://huggingface.co/metrics/wer). Мы загрузим метрику WER из 🤗 Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f4750e8-237f-4dd7-bd2d-27004238c71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d22eda24edc4df3a268d39eacda73b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "print(\"Metric is loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f2f95-0572-4f69-ac2c-b50185116464",
   "metadata": {},
   "source": [
    "Затем нам просто нужно определить функцию, которая принимает предсказания нашей модели и возвращает метрику WER. Эта функция, называемая `compute_metrics`, сначала заменяет `-100` на `pad_token_id` в `label_ids` (отменяя шаг, который мы применили в \n",
    "в коллаторе данных, чтобы правильно игнорировать подстановочные токены в потерях). Затем она декодирует прогнозы и идентификаторы меток в строки. Наконец, она вычисляет WER между прогнозами (predictions) и эталонными метками (reference labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b01b4-a27b-48bd-a0c2-dcd5cdb8da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # заменяем -100 на pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # мы не хотим группировать токены при вычислении метрики\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8caa9-e1bd-428a-8672-d4ac3f5d4387",
   "metadata": {},
   "source": [
    "### Загрузка контрольной точки предварительно обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21beb9-e548-413e-8e0e-50c80e83372f",
   "metadata": {},
   "source": [
    "Теперь давайте загрузим предварительно обученную контрольную точку модели Whisper `small`. Опять же, это тривиально благодаря использованию библиотеки 🤗 Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eee780-8dbd-432e-9bf5-3d5eb8a713df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa73d25ef6d043b5b8a218b4f30d3827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be734538f0b4a1486f510aa11bfd7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "print(\"Checkpoint of the pre-trained model has been successfully loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127c743-15cc-4410-ab15-ab0a245a0fe3",
   "metadata": {},
   "source": [
    "Переопределение аргументов генерации - никакие токены не выдаются принудительно в качестве выходов декодера (смотрите [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), никакие токены не подавляются во время генерации (дополнительно смотрите [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3e2d9-8a50-4e72-8997-083d18dd3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374310ce-c37c-4656-a0aa-f4437dd830c0",
   "metadata": {},
   "source": [
    "### Определение конфигурации обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cec3af-d2ea-4eda-b5b5-339d850d1c5e",
   "metadata": {},
   "source": [
    "На последнем этапе мы определяем все параметры, связанные с процессом обучения. Более подробную информацию об аргументах процесса обучения можно найти в разделе Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05700a-1d52-49b8-be91-8ae94e9ac4cb",
   "metadata": {},
   "source": [
    "# Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc0302-ada3-488a-a3b5-670c6c5a0d34",
   "metadata": {},
   "source": [
    "- [Fine-Tune Whisper For Multilingual ASR with Transformers (Статья)](https://huggingface.co/blog/fine-tune-whisper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
