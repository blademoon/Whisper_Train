{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b1b3eb-dfbd-41cb-9656-26f992163aae",
   "metadata": {
    "id": "81b1b3eb-dfbd-41cb-9656-26f992163aae"
   },
   "source": [
    "# Предварительная настройка среды разработки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7cfc2-ab19-4ebd-b39a-ceb906c5bbf3",
   "metadata": {
    "id": "66f7cfc2-ab19-4ebd-b39a-ceb906c5bbf3"
   },
   "source": [
    "## Глобальные параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05b20d-ecce-4884-8c70-da71d00cb32d",
   "metadata": {
    "id": "8a05b20d-ecce-4884-8c70-da71d00cb32d"
   },
   "source": [
    "Глобальные параметры работы блокнота для упрощения управления процессом обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a6b180-bf66-4eca-aa1d-ec87d9df53a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1673634003246,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "c9a6b180-bf66-4eca-aa1d-ec87d9df53a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mozilla-foundation/common_voice_13_0 ru\n",
      "Russian transcribe\n",
      "openai/whisper-small\n",
      "whisper-small-fine_tuned-ru\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"mozilla-foundation/common_voice_13_0\"          # Название датасета\n",
    "dataset_lang = \"ru\"                                            # Язык датасета\n",
    "\n",
    "model_name = \"whisper-small\"                                    # Базовая модель\n",
    "model_task = \"transcribe\"\n",
    "\n",
    "tokenizer_language = \"Russian\"                                 # Язык токенизатора\n",
    "\n",
    "\n",
    "# Автоматически формируемые параметры\n",
    "reference_model = \"openai/\" + model_name\n",
    "result_model_name = model_name + \"-fine_tuned-ru\"\n",
    "\n",
    "# Время начала и конца дешевого тарифа для трехтарифного счётчика\n",
    "start_time = \"23:00:00\"\n",
    "end_time = \"07:00:00\"\n",
    "\n",
    "# DEBUG\n",
    "print(dataset_name, dataset_lang)\n",
    "print(tokenizer_language, model_task)\n",
    "print(reference_model)\n",
    "print(result_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f46edf-2e05-4d15-be5f-b0d22aded589",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690ca4c-7bce-4514-961f-8141fcbaa681",
   "metadata": {},
   "source": [
    "### Локальная среда"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248bbf3-37c5-486d-8580-8c3098c1c108",
   "metadata": {},
   "source": [
    "##### Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcea442-8a37-47bf-8ac7-2552a044170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Установка необходимых библиотек в локальную среду\n",
    "\n",
    "# !conda install -y -c conda-forge transformers\n",
    "# !conda install -y -c conda-forge datasets\n",
    "# !conda install -y -c conda-forge librosa \n",
    "# !conda install -y -c conda-forge ffmpeg \n",
    "# !conda install -y -c conda-forge huggingface_hub \n",
    "# !conda install -y -c conda-forge tensorboard\n",
    "# !conda install -y -c conda-forge git-lfs\n",
    "# !pip install --no-input evaluate\n",
    "# !pip install --no-input jiwer\n",
    "# !pip install --no-input gradio\n",
    "# !pip install huggingface_hub[\"cli\"]\n",
    "\n",
    "print(\"Local environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a0625-f554-4efa-9fd0-0f619bdc05d5",
   "metadata": {},
   "source": [
    "#### Python Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42cd4221-569a-42ba-b2b3-f9da60021eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Тестировалось на WSL 2 Ubuntu 22.04.2 LTS\n",
    "# ! pip install transformers\n",
    "# ! pip install datasets\n",
    "# ! pip install huggingface_hub\n",
    "# ! pip install huggingface_hub[\"cli\"]\n",
    "# ! pip install evaluate\n",
    "# ! pip install gradio\n",
    "# ! pip install tensorboard\n",
    "# ! pip install git-lfs\n",
    "# ! pip install librosa\n",
    "# ! pip install ffmpeg\n",
    "# ! pip install jiwer\n",
    "\n",
    "print(\"Local environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64193d70-b7f9-4fc3-9178-dcb1a024bf1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8a934c-7089-4782-965e-9357f0fb4d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Установка необходимых библиотек для Google Collab\n",
    "\n",
    "# !add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
    "# !apt update\n",
    "# !apt install -y ffmpeg\n",
    "# !pip install datasets>=2.6.1\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install librosa\n",
    "# !pip install evaluate>=0.30\n",
    "# !pip install jiwer\n",
    "# !pip install gradio\n",
    "\n",
    "# Подключим Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "# print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "print(\"Library installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8e550-e5d6-4476-96c9-a37e30016c33",
   "metadata": {
    "id": "d8a8e550-e5d6-4476-96c9-a37e30016c33"
   },
   "source": [
    "## Проверка доступности GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed895ca-e0e7-478d-9020-f3cc7b7f937f",
   "metadata": {
    "id": "9ed895ca-e0e7-478d-9020-f3cc7b7f937f"
   },
   "source": [
    "Чтобы получить GPU, нажмите _Runtime_ -> _Change runtime type_, затем измените _Hardware accelerator_ с _None_ на _GPU_.\n",
    "\n",
    "Мы можем проверить, что нам назначен GPU, и просмотреть его характеристики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8217a839-6751-4ae2-a1a4-9544a4496546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1673634074679,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "8217a839-6751-4ae2-a1a4-9544a4496546",
    "outputId": "d2cebeb1-461e-450b-f78a-86dc294b54cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 11:26:36 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.98.01              Driver Version: 536.99       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0  On |                  Off |\n",
      "| 31%   32C    P8              42W / 450W |   1431MiB / 24564MiB |     14%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        34      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756687d-d234-4d16-990a-8d2000ef2652",
   "metadata": {
    "id": "3756687d-d234-4d16-990a-8d2000ef2652"
   },
   "source": [
    "## Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6056b6-30b5-4d17-9f8d-2158e12b57c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11439,
     "status": "ok",
     "timestamp": 1673634086106,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "9e6056b6-30b5-4d17-9f8d-2158e12b57c4",
    "outputId": "1f6eac1f-6b03-4fb2-f7ba-129ee7f57852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries/modules initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# Вспомогательные библиотеки\n",
    "import os\n",
    "import subprocess\n",
    "import os\n",
    "import multiprocessing\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import time\n",
    "\n",
    "# HuggingFace\n",
    "from huggingface_hub import notebook_login, login\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from datasets import Audio\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Evaluate\n",
    "import evaluate\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "print(\"Required libraries/modules initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecc953-8821-4064-97cc-dc17b5ccc74c",
   "metadata": {
    "id": "b9ecc953-8821-4064-97cc-dc17b5ccc74c"
   },
   "source": [
    "## Настройка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f54433-bfb3-48f9-9ce7-a980d4c0a477",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384,
     "referenced_widgets": [
      "dc0675e545f6409997809fcb6cab6165",
      "83fb9a9c5e0544d88d17dfa52669466a",
      "ce189b2f200141abbba18fc4904e357b",
      "3d3761c108ea4cf8bf5a2e3c19fbb1ff",
      "36c9544eb28b4a60b323fabbff3bb1f9",
      "01318a1365044fc49644200e3da8a7a8",
      "dc0ceb8a76814014a97c26daa245a93c",
      "1257292285e44b49986fb03281e90d60",
      "fb5b3dfa404b472fac562aad2dc11b72",
      "3041aa5c6cca44f3ac7ed149a8f695da",
      "93e2052e52d04220b5817189843da1a1",
      "892fbc008e774549a4867629fa7a80ea",
      "56431bcba52f41cf82985766df22016e",
      "8b28ba2b19314571a51689b3767bb892",
      "7193255aba194762aca8ffd22d33f1d2",
      "2b476c9798ca428fb4188efbeb068164",
      "6ba277e2abd94b4fb427ffb20c6c8bff"
     ]
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1673634086107,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "c5f54433-bfb3-48f9-9ce7-a980d4c0a477",
    "outputId": "6deb2628-eb5d-4e08-f9f7-cc8aa8caefab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of workers processes is set at 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a229c5dee64aacb54cac8d939d3653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup completed!\n"
     ]
    }
   ],
   "source": [
    "# Определим количество ядер CPU, это влияет на количество рабочих процессов которые мы можем запустить одновременно\n",
    "system_num_workers = multiprocessing.cpu_count()\n",
    "print(\"The number of workers processes is set at\", system_num_workers)\n",
    "\n",
    "# Отключение назойлевых предупреждений\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Аутентификация ноутбука в HuggingFace HUB\n",
    "notebook_login()\n",
    "\n",
    "print(\"Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e653117-9a0f-4638-a647-d2ac069c795d",
   "metadata": {},
   "source": [
    "## Определение вспомогательных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31620117-d7ca-4139-8185-834392317a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вспомогательные функции определены.\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# from time import sleep\n",
    "\n",
    "# def operates_during_cheap_electricity_tariffs(start_time, end_time):\n",
    "\n",
    "#     # Функция для проверки укладывается ли текущее время в диапазон дешевого тарифа\n",
    "#     def in_between(now, start, end):\n",
    "#         if start <= end:\n",
    "#             return start <= now < end\n",
    "#         else: # over midnight e.g., 23:30-04:15\n",
    "#             return start <= now or now < end\n",
    "\n",
    "#     # Функций возвращающая текущее время timedelta\n",
    "#     def get_now():\n",
    "#         now = (datetime.datetime.now()).time()\n",
    "#         td_now = datetime.timedelta(hours=now.hour, minutes=now.minute, seconds=now.second)\n",
    "#         return td_now\n",
    "\n",
    "#     # Функция конвертирующая время суток в 24-часово формате в timedelta\n",
    "#     def convert_str_timedelta(time):\n",
    "#         result = (datetime.datetime.strptime(time, \"%H:%M:%S\")).time() \n",
    "#         result = datetime.timedelta(hours=result.hour, minutes=result.minute, seconds=result.second)\n",
    "#         return result\n",
    "    \n",
    "#     td1 = convert_str_timedelta(start_time)\n",
    "#     td2 = convert_str_timedelta(end_time)\n",
    "#     td_now = get_now()\n",
    "\n",
    "#     if not in_between(td_now, td1, td2):\n",
    "#         time_wait = (td1 - td_now)\n",
    "#         print(\"Current time:\", td_now ,\" Work has been suspended until the time of the cheaper electricity tariff at\", start_time)\n",
    "#         sleep(time_wait.total_seconds())\n",
    "\n",
    "#         td_now = get_now()\n",
    "#         print(\"Current time:\", td_now ,\"Work resumed.\")\n",
    "\n",
    "# print(\"Вспомогательные функции определены.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147ebd1-58e2-4ad7-a397-24e6805bd1a0",
   "metadata": {
    "id": "6147ebd1-58e2-4ad7-a397-24e6805bd1a0"
   },
   "source": [
    "# Тонкая настройка Whisper для многоязыкового ASR с помощью библиотеки 🤗 Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca114a35-8e37-4e5d-bd57-165a43b8f79c",
   "metadata": {
    "id": "ca114a35-8e37-4e5d-bd57-165a43b8f79c"
   },
   "source": [
    "В этом блокноте мы представляем пошаговое руководство по тонкой настройке Whisper для любого многоязыкового набора данных ASR с помощью библиотеки Hugging Face 🤗 Transformers. Это более \"практическая\" версия сопутствующего [сообщения в блоге](https://huggingface.co/blog/fine-tune-whisper). Для более подробного объяснения Whisper, набора данных Common Voice и теории, лежащей в основе тонкой настройки, читателю рекомендуется обратиться к статье в блоге."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b0dcf-f121-407d-8688-ae7eed7bff9c",
   "metadata": {
    "id": "596b0dcf-f121-407d-8688-ae7eed7bff9c"
   },
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5234d-999c-4ea6-aef6-a6a8f2fae199",
   "metadata": {
    "id": "75a5234d-999c-4ea6-aef6-a6a8f2fae199"
   },
   "source": [
    "Whisper - это предварительно обученная модель для автоматического распознавания речи (ASR) опубликованная в [сентябре 2022](https://openai.com/blog/whisper/) авторами Алеком Рэдфордом и другими из OpenAI. В отличие от многих своих предшественников, таких как. \n",
    "[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), которые предварительно обученны на неразмеченных аудиоданных, Whisper предварительно обучен на огромном количестве **размеченных** транскрибированных аудио данных, 680 000 часов, если быть точным. Это на порядок больше данных, чем использованные для обучения Wav2Vec 2.0 (60 000 часов) неразмеченных аудиоданных. Более того, 117 000 часов из этих данных использованных для предварительного обучения - это многоязыковые данные ASR. Это позволяет получить контрольные точки которые могут быть применены к более чем 96 языкам, многие из которых считаются _низкоресурсными_.\n",
    "\n",
    "При масштабировании на 680 000 часов размеченных данных предварительного обучения, модели Whisper демонстрируют высокую способность к обобщению для многих наборов данных и областей (доменов). Предварительно обученные контрольные точки достигают результатов, конкурентоспособных с современными \n",
    "ASR системами, с коэффициентом ошибок в словах (WER) около 3% на подмножестве чистых тестов LibriSpeech ASR и новым передовым результатом на TED-LIUM с 4,7% (смотрите таблицу 8 из [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)). \n",
    "\n",
    "Обширные знания о многоязычном ASR, полученные Whisper во время предварительного обучения могут быть использованы для других языков с низким уровнем ресурсов; посредством тонкой настройки предварительно обученные контрольные точки могут быть адаптированы для конкретных наборов данных и языков для дальнейшего улучшения результатов. Мы покажем, как можно точно настроить Whisper для языков с ограниченными ресурсами в этом блокноте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b4cc4-1ec3-4857-b11f-8aff83c0e06f",
   "metadata": {
    "id": "e32b4cc4-1ec3-4857-b11f-8aff83c0e06f"
   },
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/whisper_architecture.svg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Рисунок 1:</b> модель Whisper. Архитектура соответствует стандартной модели кодера-декодера на основе трансформатора. Log-Mel спектрограмма подается на вход кодера. Последние скрытые состояния поступают в декодер через механизмы перекрестного внимания. Декодер авторегрессионно предсказывает текстовые токены, совместно обусловленные скрытыми состояниями энкодера и ранее предсказанными токенами. Источник рисунка: \n",
    "<a href=\"https://openai.com/blog/whisper/\">блог OpenAI Whisper</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731f8d5-ebd9-4a90-9564-a1b91ea5d890",
   "metadata": {
    "id": "a731f8d5-ebd9-4a90-9564-a1b91ea5d890"
   },
   "source": [
    "Контрольные точки Whisper представлены в пяти конфигурациях с различными размерами моделей. Самые маленькие четыре модели обучаются либо только на английском, либо на многоязычных данных. Самая большая контрольная точка - только многоязычная. \n",
    "Все девять предварительно обученных контрольных точек доступны на сайте [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). Контрольные точки сведены в следующую таблицу со ссылками на модели в хабе:\n",
    "\n",
    "| Размер   | Количество слоёв | Ширина | Количество голов | Параметры | Только англоязычная модель                                         | Многоязычная модель                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "В демонстрационных целях мы доработаем контрольную точку многоязычной версии [``малой модели``](https://huggingface.co/openai/whisper-small) с 244M параметрами (~= 1GB). Что касается наших данных, мы обучим и оценим нашу систему на языке с низким уровнем ресурсов, взятом из набора данных [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Мы покажем, что всего за 8 часов тонкой настройки данных мы можем добиться высоких результатов на этом языке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d7e44-a8a4-4640-a07e-6ecb1b9f5bd0",
   "metadata": {
    "id": "a46d7e44-a8a4-4640-a07e-6ecb1b9f5bd0"
   },
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "${}^1$ - Название Whisper происходит от аббревиатуры \"WSPSR\", которая расшифровывается как \"Web-scale Supervised Pre-training for Speech Recognition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da24b6-3178-4538-8cd5-57ff8b22aaba",
   "metadata": {
    "id": "85da24b6-3178-4538-8cd5-57ff8b22aaba"
   },
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90357e66-1041-4c4f-9e7e-d40d94cae25b",
   "metadata": {
    "id": "90357e66-1041-4c4f-9e7e-d40d94cae25b"
   },
   "source": [
    "Используя 🤗 Datasets, загрузка и подготовка данных чрезвычайно проста. \n",
    "\n",
    "Мы можем загрузить и подготовить сплиты Common Voice всего одной строкой кода. \n",
    "\n",
    "Во-первых, убедитесь, что вы приняли условия использования на Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). После принятия условий вы получите полный доступ к набору данных и сможете загружать данные локально.\n",
    "\n",
    "Объединим `train` и `validation` части набора данных, чтобы получить как можно больше данных для обучения. Будем использовать\n",
    "данных `test` в качестве нашего тестового набора.\n",
    "\n",
    "> ВАЖНО!\n",
    "> Для удобства хранения данных датасета, я ранее определил путь к папке в переменной `DATA_PATH`. Это позволит хранить кеш данных в известном заранее месте и в дальшнейшем удалить ненужные данные. Это позволит быстро освободить место на HDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbccf612-70ea-4022-8a12-d3ba7cd21d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/artyom/.cache/huggingface/datasets/artyomboyko___parquet/artyomboyko--common_voice_13_0_ru_dataset_for_whisper_fine_tune-42b90f359d1e87c7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70244fa15f0f438293e24de6e4bc0b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 36454\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 10186\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice =  load_dataset(\"artyomboyko/common_voice_13_0_ru_dataset_for_whisper_fine_tune\")\n",
    "\n",
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82d17d-69bc-4e13-bdf1-4a058c367733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450,
     "referenced_widgets": [
      "7b8a33c170a544c6b16091753b3a5054",
      "00c53984754d4dab9158434bbd37523c",
      "da61ea52e72549db962c38b3f71cdea5",
      "40d73b56aeab4ee79212f8a3f3b0223b",
      "321707defc904da7a2baa6a0ffd454bd",
      "23f5b55fead94386a3693c43547eb251",
      "dbeda59754d747678b35d2f998379704",
      "14517348c1544171804ab98ad6f25af8",
      "68a5f2e539cb4f08869923b400bdb40a",
      "e9424928a29a483d871ff5c9337e016f",
      "5fa9c0392c034feda1b5e77c02e258ce"
     ]
    },
    "executionInfo": {
     "elapsed": 122279,
     "status": "error",
     "timestamp": 1673634383009,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "5a82d17d-69bc-4e13-bdf1-4a058c367733",
    "outputId": "c398c73f-84ce-4c15-d68c-25ad38e79bcf"
   },
   "outputs": [],
   "source": [
    "# Создадим словарь для хранения датасета\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, dataset_lang, split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, dataset_lang, split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)\n",
    "print(\"Data download successfully completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a920f22-3ede-492d-a7eb-9e764d3241c1",
   "metadata": {
    "id": "5a920f22-3ede-492d-a7eb-9e764d3241c1"
   },
   "source": [
    "Большинство наборов данных ASR предоставляют только входные образцы аудио (`audio`) и соответствующий транскрибированный текст (`sentence`).  Common Voice содержит дополнительную метаданные, такие как `accent` и `locale`, которые мы можем игнорировать для ASR.\n",
    "\n",
    "Оставив блокнот максимально общим, мы рассматриваем только входной звук и транскрибированный текст для тонкой настройки, отбрасывая дополнительную информацию метаданных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d79484dc-05a9-42b6-aed9-5289b0c60a58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1673634091793,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "d79484dc-05a9-42b6-aed9-5289b0c60a58",
    "outputId": "89221b12-015b-464a-c975-0c1586496b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence', 'variant'],\n",
      "        num_rows: 36454\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence', 'variant'],\n",
      "        num_rows: 10186\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b572fa-b08d-4e94-8d20-6c341f23d1da",
   "metadata": {
    "id": "65b572fa-b08d-4e94-8d20-6c341f23d1da"
   },
   "source": [
    "## Подготовка экстрактора признаков, токенизатора и данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e3a16-2c58-4e1c-808c-985adb82f22e",
   "metadata": {
    "id": "832e3a16-2c58-4e1c-808c-985adb82f22e"
   },
   "source": [
    "Конвейер ASR можно разделить на три этапа: \n",
    "1) экстрактор признаков, который предварительно обрабатывает необработанные аудиоданные\n",
    "2) Модель, которая выполняет сопоставление последовательности с последовательностью \n",
    "3) Токенизатор, который постобрабатывает выводы модели в текстовый формат.\n",
    "\n",
    "Модель Whisper из библиотеки 🤗 Transformers имеет ассоциированный экстрактор признаков и токенизатор, называемые [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
    "и [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer) \n",
    "соответственно.\n",
    "\n",
    "Рассмотрим детальнее настройки экстрактора и токенизатора по очереди!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e8c78-2986-4597-850d-5cbfcfdcdf78",
   "metadata": {
    "id": "1b6e8c78-2986-4597-850d-5cbfcfdcdf78"
   },
   "source": [
    "### Загрузка WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da804fb-d0f5-49ae-8b4b-c370f135286a",
   "metadata": {
    "id": "4da804fb-d0f5-49ae-8b4b-c370f135286a"
   },
   "source": [
    "Экстрактор признаков Whisper выполняет две операции:\n",
    "1. Разбивка / усечение аудиоданных до 30 секунд: все аудиоданные короче 30 секунд разбиваются на молчание (нули), а те, что длиннее 30 секунд, усекаются до 30 секунд.\n",
    "2. Преобразует входные аудио в _log-Mel спектрограммы_ входных признаков, визуальное представление аудио и форму входных данных, ожидаемых моделью Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c57886-db51-4867-86d8-33495e18d5b3",
   "metadata": {
    "id": "56c57886-db51-4867-86d8-33495e18d5b3"
   },
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Рисунок 2:</b> Преобразование дискретизированного аудио массива в log-Mel спектрограмму.\n",
    "Слева: дискретизированный одномерный аудиосигнал. Справа: соответствующая log-Mel спектрограмма. Источник рисунка:\n",
    "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eabd2e-88c6-4617-8296-21f26e3dfc8e",
   "metadata": {
    "id": "02eabd2e-88c6-4617-8296-21f26e3dfc8e"
   },
   "source": [
    "Мы загрузим экстрактор признаков из предварительно обученной контрольной точки со параметрами по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252733c8-7c82-4096-8d25-7d7b4b631077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переопределим модель для проведения дообучения \n",
    "reference_model = \"artyomboyko/whisper-small-fine_tuned-ru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a4b5d8-b946-48d0-b349-85db08a1968b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1673634091793,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "72a4b5d8-b946-48d0-b349-85db08a1968b",
    "outputId": "1d05f879-6c27-44ec-abec-94ae7de76583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature extractor is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(reference_model)\n",
    "\n",
    "print(\"The feature extractor is loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf53f7-bec1-44ca-b1db-1aa395cef30f",
   "metadata": {
    "id": "aeaf53f7-bec1-44ca-b1db-1aa395cef30f"
   },
   "source": [
    "### Загрузка WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d921f8-b3a4-4afc-8d67-35281063e47e",
   "metadata": {
    "id": "17d921f8-b3a4-4afc-8d67-35281063e47e"
   },
   "source": [
    "Модель Whisper выдает последовательность идентификаторов _токенов_. Токенизатор сопоставляет каждый из этих идентификаторов токенов с соответствующей текстовой строкой. Для русского языка мы можем загрузить предварительно обученный токенизатор и использовать его для тонкой настройки \n",
    "с указанием целевого языка и задачи. Эти аргументы сообщают токенизатору о том, что следует задавать префикс токена языка и задачи в начале последовательности меток:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3ad868-efde-495e-82aa-780f5c4b9078",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1673634092276,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "6a3ad868-efde-495e-82aa-780f5c4b9078",
    "outputId": "8dac6c32-85df-425f-a651-f37dd4219300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(reference_model,\n",
    "                                             language = tokenizer_language,\n",
    "                                             task = model_task)\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434f3bf-5240-4650-9ca7-57edec2a59dd",
   "metadata": {
    "id": "5434f3bf-5240-4650-9ca7-57edec2a59dd"
   },
   "source": [
    "### Комбинирование для создания WhisperProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d28df3-e0de-499d-8141-991d1785282f",
   "metadata": {
    "id": "b6d28df3-e0de-499d-8141-991d1785282f"
   },
   "source": [
    "Чтобы упростить использование экстрактора признаков и токенизатора, мы можем _обернуть_ их в один класс `WhisperProcessor`. Этот объект наследуется от `WhisperFeatureExtractor` и `WhisperProcessor`, и может использоваться для обработки входных аудиоданных и \n",
    "получения предсказаний модели по мере необходимости. При этом во время обучения нам нужно отслеживать только два объекта: `processor` и `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef4a58f-393e-4a6f-8813-fa3d7904b961",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1934,
     "status": "ok",
     "timestamp": 1673634094208,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "7ef4a58f-393e-4a6f-8813-fa3d7904b961",
    "outputId": "e9ee2ba9-8bb2-4c6b-ebac-6239e70b3dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(reference_model, \n",
    "                                             language = tokenizer_language,\n",
    "                                             task = model_task)\n",
    "\n",
    "print(\"Processor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a495cd5-cd68-4cb5-934b-4a0d885c38c1",
   "metadata": {
    "id": "1a495cd5-cd68-4cb5-934b-4a0d885c38c1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfe667-ebeb-43cf-8bed-14a666eb7dec",
   "metadata": {
    "id": "81cfe667-ebeb-43cf-8bed-14a666eb7dec"
   },
   "source": [
    "Давайте распечатаем первый образец набора данных Common Voice, чтобы посмотреть в какой форме находятся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb74a1-99c2-4165-a151-f03a9b2b044e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1673634094827,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "80eb74a1-99c2-4165-a151-f03a9b2b044e",
    "outputId": "357c2aa5-907d-4392-ac94-95ab84a6c9ac"
   },
   "outputs": [],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250c661-248c-48f0-bb0c-0097d6455c73",
   "metadata": {
    "id": "a250c661-248c-48f0-bb0c-0097d6455c73"
   },
   "source": [
    "Поскольку наш входной звук сэмплирован с частотой 48 кГц (`'...sampling_rate': 48000...`), нам нужно _уменьшить_ его до частоту дискретизации до 16 кГц перед тем, как передать его в экстрактор признаков Whisper, 16 кГц - это частота дискретизации, ожидаемая моделью Whisper. Мы установим для аудио входов правильную частоту дискретизации с помощью метода [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column). Эта операция не изменяет звук \"на месте\", а скорее дает сигнализирует `datasets` передискретизировать аудиосэмплы _на лету_ при при первой загрузке данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d59a5789-c125-4371-96c5-81f7d345774d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1673634094827,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "d59a5789-c125-4371-96c5-81f7d345774d",
    "outputId": "66110b5b-f12d-485a-e671-8eb5694af79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample rate set successfully.\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(\"Audio sample rate set successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691d753-3d67-4724-bc08-d8018ba66795",
   "metadata": {
    "id": "6691d753-3d67-4724-bc08-d8018ba66795"
   },
   "source": [
    "Повторная загрузка первого аудиообразца из набора данных Common Voice приведет к его передискретизации до нужной частоты дискретизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06e51d92-dd89-4690-ae1d-aa9cc358163e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1673634094828,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "06e51d92-dd89-4690-ae1d-aa9cc358163e",
    "outputId": "cc898f66-1cf0-4676-d379-db48462f50b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/artyom/.cache/huggingface/datasets/downloads/extracted/a89420fafbe263300e50be6d61d5d75633b434150dc09fe5f7cde741f4306e05/ru_train_0/common_voice_ru_22070781.mp3', 'array': array([ 2.91038305e-11, -2.91038305e-11,  8.00355338e-11, ...,\n",
      "        1.80914412e-08,  8.51605364e-09, -5.54000508e-08]), 'sampling_rate': 16000}, 'sentence': 'Я рад, что Генеральная Ассамблея начала ее обсуждение.', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36b2eb-851f-4f6f-9222-18d24a18de87",
   "metadata": {
    "id": "2c36b2eb-851f-4f6f-9222-18d24a18de87"
   },
   "source": [
    "Теперь мы можем написать функцию для подготовки наших данных к работе с моделью:\n",
    "1. Мы загружаем и передискретизируем аудиоданные, вызывая `batch[\"audio\"]`. Как объяснялось выше, 🤗 Datasets выполняет все необходимые операции по передискретизации на лету.\n",
    "2. Мы используем экстрактор признаков для вычисления входных признаков спектрограммы log-Mel из нашего одномерного аудио массива.\n",
    "3. Мы кодируем транскрипции в идентификаторы меток с помощью токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f51abe62-7257-4363-8e15-6abfbe65cab9",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1673634094828,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "f51abe62-7257-4363-8e15-6abfbe65cab9"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Загрузим и передискретизируем аудиоданные с 48 до 16 кГц\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # вычислим log-Mel входные признаки из входного аудио массива  \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # закодируем целевой текст в идентификаторы меток\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0f12d-1dc0-4b09-9a0e-36883e91231e",
   "metadata": {
    "id": "61c0f12d-1dc0-4b09-9a0e-36883e91231e"
   },
   "source": [
    "Мы можем применить функцию подготовки данных ко всем нашим учебным примерам, используя метод `.map` в наборе данных. Аргумент `num_proc` указывает, сколько ядер процессора использовать. Если задать `num_proc` > 1, то будет включена многопроцессорная обработка. Если метод `.map` зависает при многопроцессорной обработке, установите `num_proc=1` и обрабатывайте набор данных последовательно.\n",
    "\n",
    "> ВАЖНО!    \n",
    "> Для удобства работы я использую автоматическое определения количества рабочих процессов через переменную `system_num_workers`. Это позволяет существенно повысить производительность и сэкономить время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "911c267f-d85d-4e47-8f40-549ce904ce05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12285,
     "status": "ok",
     "timestamp": 1673634107109,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "911c267f-d85d-4e47-8f40-549ce904ce05",
    "outputId": "5f7d1463-7770-482a-c38f-527fb7b424a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number of CPU cores is 20.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The current number of CPU cores is {}.\".format(system_num_workers))\n",
    "\n",
    "# common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc = system_num_workers)\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2281d-ffb5-4e9a-9464-a7e263925c1d",
   "metadata": {},
   "source": [
    "#### Сохранение обработанных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624a829-8ae6-44ce-a58a-bd07b9a7b1e6",
   "metadata": {},
   "source": [
    "Чтобы не тратить время на предварительную подготовку данных, созраним подготовленный датасет на диск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c24fcf1b-5596-4d2d-9d70-db4e5c9a85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -v \"PREPARED_DATASET\"\n",
    "\n",
    "prepared_dataset_path = \"./PREPARED_DATASET\"\n",
    "\n",
    "common_voice.save_to_disk(prepared_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ca2fe-6e71-44d2-afe0-768bf7ab58dc",
   "metadata": {},
   "source": [
    "#### Загрузка ранее предобработанных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b3573-7a69-49d2-b31f-baa422fef1a4",
   "metadata": {},
   "source": [
    "Загрузим ранее предобработанный датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51484d4d-03d9-492b-847d-9af7c32fc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset_path = \"./PREPARED_DATASET\"\n",
    "\n",
    "common_voice = load_from_disk(prepared_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3add02d-143c-4883-8695-1268b4b68d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b781aaa-1077-42d8-9a77-5d634db60f88",
   "metadata": {
    "id": "5b781aaa-1077-42d8-9a77-5d634db60f88"
   },
   "source": [
    "## Обучение и оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1beb657-2884-480c-9a1b-5529d039b6c1",
   "metadata": {
    "id": "b1beb657-2884-480c-9a1b-5529d039b6c1"
   },
   "source": [
    "Теперь, когда мы подготовили наши данные, мы готовы погрузиться в конвейер обучения. [🤗Тренер (Trainer)](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) сделает за нас большую часть тяжелой работы. Все, что нам нужно сделать, это:\n",
    "- Определить коллатор данных (data collator): коллатор данных берет наши предварительно обработанные данные и подготавливает тензоры PyTorch, готовые для модели.\n",
    "- Выбрать метрику для оценки: во время оценки мы хотим оценить модель с помощью метрики [word error rate (WER)](https://huggingface.co/metrics/wer). Нам нужно определить функцию `compute_metrics`, которая будет обрабатывать эти вычисления.\n",
    "- Загрузить предварительно обученную контрольную точку: нам нужно загрузить предварительно обученную контрольную точку и правильно настроить ее для обучения.\n",
    "- Определить конфигурацию процесса обучения: она будет использоваться 🤗 тренером для определения параметров тренировок.\n",
    "\n",
    "После того как мы произведем тонкую настройку модели, мы оценим ее на тестовых данных, чтобы убедиться, что мы правильно обучили ее транскрибировать речь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5882122-798a-4cd2-b0f8-ec5c290c49ea",
   "metadata": {
    "id": "a5882122-798a-4cd2-b0f8-ec5c290c49ea"
   },
   "source": [
    "### Определение коллатора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78853c4-bae6-46c9-8536-ae7bc8663f5f",
   "metadata": {
    "id": "b78853c4-bae6-46c9-8536-ae7bc8663f5f"
   },
   "source": [
    "Коллатор данных для модели последовательной речи уникален в том смысле, что он обрабатывает входные признаки `input_features` и метки `labels` независимо друг от друга: входные признаки `input_features` должны быть обработанны экстрактором признаков, а метки `labels` - токенизатором. \n",
    "\n",
    "Входные признаки `input_features` уже разбиты на аудиофрагменты длительностью 30 секунд и преобразованы в соответствующие им спектрограммы log-Mel фиксированной размерности с помощью экстрактора признаков, поэтому все, что нам нужно сделать, это преобразовать входные признаки `input_features` в тензоры PyTorch. Мы делаем это с помощью метода `.pad` экстрактора признаков с хаданием параметра `return_tensors=pt`.\n",
    "\n",
    "С другой стороны, `метки (labels)` не имеют вставок (un-padded). Сначала мы выравниваем последовательности до максимальной длины в батче, используя метод `.pad` токенизатора. Затем токены-вставки заменяются на значение `-100`, чтобы эти токены **не** учитывались при вычислении потерь. Затем мы вырезаем токен BOS из начала последовательности меток, так как мы добавим его позже во время обучения. \n",
    "\n",
    "Мы можем использовать препроцессор `WhisperProcessor`, который мы определили ранее, для выполнения извлечения признаков, так и для работы с токенами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9a116b-8b60-451b-b274-f177c6adcb52",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1673634107109,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "7d9a116b-8b60-451b-b274-f177c6adcb52"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # разделить входы и метки, так как они должны быть разной длины и нуждаться в разных методах дополнения (padding)\n",
    "        # сначала обрабатываем аудиовходы, просто возвращая тензоры torch\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # получим токенизированные последовательности меток\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # заменим дополнение (padding) на -100, чтобы корректно игнорировать потери\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # если токен bos был добавлен на предыдущем шаге токенизации,\n",
    "        # вырезаем здесь токен bos, так как он все равно будет добавлен позже\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c9591-cda5-47fb-8438-7d0d1859fc08",
   "metadata": {
    "id": "c40c9591-cda5-47fb-8438-7d0d1859fc08"
   },
   "source": [
    "Давайте инициализируем коллатор данных, который мы только что определили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "682bd554-b814-457e-8239-2d9db732c81f",
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1673632342686,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "682bd554-b814-457e-8239-2d9db732c81f"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f1ac2-419e-4e34-b6c8-96dee3ede7a8",
   "metadata": {
    "id": "756f1ac2-419e-4e34-b6c8-96dee3ede7a8"
   },
   "source": [
    "### Метрика оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689a5a1-64ba-4fe9-aac4-f2c7859f2ed7",
   "metadata": {
    "id": "c689a5a1-64ba-4fe9-aac4-f2c7859f2ed7"
   },
   "source": [
    "Мы будем использовать метрику коэффициента ошибок в словах (WER), которая является \"де-факто\" метрикой для оценки систем ASR-систем. За дополнительной информацией обратитесь к документу WER [docs](https://huggingface.co/metrics/wer). Мы загрузим метрику WER из 🤗 Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4750e8-237f-4dd7-bd2d-27004238c71c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a2532cb8ce974839918aff4a99e9a28f",
      "6d51bbda960144d78cba1585cbe4a644",
      "7aa6f6bc2aed4420907939a6fdb34446",
      "6abd5d7f9c2248ed85d4b7f61dd11e6c",
      "57d3dfb754a74770b50fa69bb7eb9e5b",
      "9edc322700ed4ce8bf234e28ced75617",
      "b653be6fc7114033bcc2afd122f61302",
      "79756643ded74db99294cb00275f2aa7",
      "ae78207e9d5d439cab2a44e93e2ae85c",
      "1af9316db9204ec899077819ac4a33e2",
      "9489fa39837a4f05812064d4b0b1f83f"
     ]
    },
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1673632256458,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "4f4750e8-237f-4dd7-bd2d-27004238c71c",
    "outputId": "9a19e114-96b5-4c22-f047-1c4a7f3b91c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "#metric_cer = evaluate.load(\"cer\")\n",
    "\n",
    "print(\"Metric is loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f2f95-0572-4f69-ac2c-b50185116464",
   "metadata": {
    "id": "242f2f95-0572-4f69-ac2c-b50185116464"
   },
   "source": [
    "Затем нам просто нужно определить функцию, которая принимает предсказания нашей модели и возвращает метрику WER. Эта функция, называемая `compute_metrics`, сначала заменяет `-100` на `pad_token_id` в `label_ids` (отменяя шаг, который мы применили в \n",
    "в коллаторе данных, чтобы правильно игнорировать подстановочные токены в потерях). Затем она декодирует прогнозы и идентификаторы меток в строки. Наконец, она вычисляет WER между прогнозами (predictions) и эталонными метками (reference labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051b01b4-a27b-48bd-a0c2-dcd5cdb8da0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1673632271713,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "051b01b4-a27b-48bd-a0c2-dcd5cdb8da0c",
    "outputId": "499771ba-d589-4785-9960-9484f12af6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compute_metrics function was defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # заменяем -100 на pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # мы не хотим группировать токены при вычислении метрики\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    #cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    #operates_during_cheap_electricity_tariffs(start_time, end_time)\n",
    "\n",
    "    #return {\"wer\": wer, \"cer\": cer}\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "print(\"The compute_metrics function was defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8caa9-e1bd-428a-8672-d4ac3f5d4387",
   "metadata": {
    "id": "11c8caa9-e1bd-428a-8672-d4ac3f5d4387"
   },
   "source": [
    "### Загрузка контрольной точки предварительно обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21beb9-e548-413e-8e0e-50c80e83372f",
   "metadata": {
    "id": "3c21beb9-e548-413e-8e0e-50c80e83372f"
   },
   "source": [
    "Теперь давайте загрузим предварительно обученную контрольную точку модели Whisper `small`. Опять же, это тривиально благодаря использованию библиотеки 🤗 Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "733e0b3c-8c35-4f89-81b6-1233e953f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = \"artyomboyko/whisper-small-fine_tuned-ru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6eee780-8dbd-432e-9bf5-3d5eb8a713df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "3a77a46f4b22491581b4811abf73634d",
      "b4dd8b85931b476caf5fce8b899d9e5a",
      "01e430b67361463d8c70a1943f6a06e2",
      "168764340c704fdf98cf343f9fbe3e6d",
      "ebe830bca86b47fcbc3a466a0f453b1d",
      "45031fd2b0344c6ca326286e2f8b5315",
      "ae9c664698a74e6ca277496fbbc96a9d",
      "b53ab33375824d4690ac58d4c6ccf5c5",
      "ea3bbe1e18294d2882dcf889403cacfe",
      "a0c281f2a8a84e43b42d121604a3c4aa",
      "254c0051d04c404fac8ba0ab4f4d0f8e",
      "f79a3e34697145c2829d0017fe14f62a",
      "caa97ce42d284b2aabc42a6cf53a6733",
      "eb53afde68d0440ba4c013e517c5f6ea",
      "b2d47415bb924f6b927765131d9f0e12",
      "7ed67d56fce94d34b9393e396344809c",
      "665a3db7544149ba955f1ea4de5552ef",
      "5ea0ba0d6c944625a918e26e5f7e44a4",
      "f658de7c58c04e9bae9fb25702688d20",
      "af49c1f86fe24459b045603b4e924a5a",
      "f277dce872d64621883205fd1217345c",
      "01eab296e5e24f8f976b80d9cc9c5d27"
     ]
    },
    "executionInfo": {
     "elapsed": 24850,
     "status": "ok",
     "timestamp": 1673632298715,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "f6eee780-8dbd-432e-9bf5-3d5eb8a713df",
    "outputId": "d76e1a67-1c4b-4adc-f5fb-6dca907073d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current reference model: artyomboyko/whisper-small-fine_tuned-ru\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cab712885040e6b189ed684d26c71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint of the pre-trained model has been successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Current reference model:\", reference_model)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(reference_model)\n",
    "\n",
    "print(\"Checkpoint of the pre-trained model has been successfully loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127c743-15cc-4410-ab15-ab0a245a0fe3",
   "metadata": {
    "id": "3127c743-15cc-4410-ab15-ab0a245a0fe3"
   },
   "source": [
    "Переопределение аргументов генерации - никакие токены не выдаются принудительно в качестве выходов декодера (смотрите [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), никакие токены не подавляются во время генерации (дополнительно смотрите [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf3e2d9-8a50-4e72-8997-083d18dd3f73",
   "metadata": {
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1673632308331,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "cbf3e2d9-8a50-4e72-8997-083d18dd3f73"
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374310ce-c37c-4656-a0aa-f4437dd830c0",
   "metadata": {
    "id": "374310ce-c37c-4656-a0aa-f4437dd830c0"
   },
   "source": [
    "### Определение конфигурации обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cec3af-d2ea-4eda-b5b5-339d850d1c5e",
   "metadata": {
    "id": "70cec3af-d2ea-4eda-b5b5-339d850d1c5e"
   },
   "source": [
    "На последнем этапе мы определяем все параметры, связанные с процессом обучения. Более подробную информацию об аргументах процесса обучения можно найти в разделе Seq2SeqTrainingArguments [документации](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments).\n",
    "\n",
    "> **Примечание**: если вы не хотите загружать контрольные точки модели в хаб HunggingFace, установите параметр `push_to_hub=False`.\n",
    "\n",
    "> **ПРИМЕЧАНИЕ**:\n",
    "> Здесь я разделяю настройки тренера для модели на два варианта:\n",
    "> - Локальная среда, используется для обучения моделей на локальном ПК,\n",
    "> - Google Colab, так как в нем доступны GPU с большим обьемом VRAM бесплатно,\n",
    ">\n",
    "> ***Оригиниальные настройки обучения модели сохранены в разделе Google Colab!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866885f3-b32e-400d-82aa-5584ebbf21b6",
   "metadata": {},
   "source": [
    "#### Локальная среда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78e85045-0327-4bcd-a799-025a4117a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = result_model_name,      # измените имя репозитория по своему усмотрению\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,       # увеличивается в 2x раза при каждом уменьшении размера батча в 2x раза  \n",
    "    learning_rate=2e-7,\n",
    "    warmup_steps=250,\n",
    "    max_steps=10000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    optim = \"adamw_torch\",\n",
    "    per_device_eval_batch_size=8,        # Самый простой способ задать равным параметру per_device_train_batch_size\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc63e97-bca2-4120-a342-e80afc2046c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d081f13c-21b6-4ff5-bb66-7b6e873a52bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1673633946342,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "d081f13c-21b6-4ff5-bb66-7b6e873a52bb",
    "outputId": "2f4ea0c0-d7e6-4947-a363-70491861bf82"
   },
   "outputs": [],
   "source": [
    "# from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-base-ru\",       # измените имя репозитория по своему усмотрению\n",
    "#     per_device_train_batch_size=16,\n",
    "#     gradient_accumulation_steps=1,         # увеличивается в 2x раза при каждом уменьшении размера батча в 2x раза\n",
    "#     learning_rate=1e-6,\n",
    "#     warmup_steps=500,\n",
    "#     max_steps=4000,\n",
    "#     gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_eval_batch_size=8,          # Самый простой способ задать равным параметру per_device_train_batch_size\n",
    "#     predict_with_generate=True,\n",
    "#     generation_max_length=225,\n",
    "#     save_steps=1000,\n",
    "#     eval_steps=1000,\n",
    "#     logging_steps=25,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"wer\",\n",
    "#     greater_is_better=False,\n",
    "#     push_to_hub=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737a5e-223a-45c9-9d28-827c65b40a27",
   "metadata": {},
   "source": [
    "### Задание настроек процесса обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4d169-8852-44c5-85b9-d81e43414e60",
   "metadata": {
    "id": "59b4d169-8852-44c5-85b9-d81e43414e60"
   },
   "source": [
    "Теперь мы можем передать аргументы обучения в 🤗 Trainer вместе с нашей моделью, набором данных, коллатором данных и функцией `compute_metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d7c4326-235c-4a53-806d-4d1b1d0a48a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4506,
     "status": "ok",
     "timestamp": 1673633952912,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "0d7c4326-235c-4a53-806d-4d1b1d0a48a0",
    "outputId": "e3a1ea94-78ec-4692-bfec-8066a2261fc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/artyomboyko/whisper-small-fine_tuned-ru into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931c6c7cf40e4a5b9d55d7030b66091b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 8.00k/922M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67399dab13464f85b50aed38ffc59ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Aug28_18-20-53_MSK-PC-01/events.out.tfevents.1693236329.MSK-PC-01.744.0:  82%|########2 | 3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bbf320607e4792a7fe656c235c4961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Aug28_18-20-53_MSK-PC-01/events.out.tfevents.1693236329.MSK-PC-01.744.0:   3%|2         | 1.00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee453e485dc2488dacfe6ad075029c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Aug16_08-45-51_MSK-PC-01/events.out.tfevents.1692164873.MSK-PC-01.484.0:  23%|##2       | 8…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e4bfb7e3564bb19e81bb4648e4ad66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file model.safetensors:   0%|          | 8.00k/922M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06280dea2ea6416fb6fcefad8dcd8313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Aug16_08-45-51_MSK-PC-01/events.out.tfevents.1692164873.MSK-PC-01.484.0:   3%|2         | 1.00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c12d3267ae413fa6c788f6d0559e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 4.00k/4.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a25b7d17814f6cbaf789db00075bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  25%|##5       | 1.00k/4.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621d0104f67540f798f60622139b2acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Aug29_13-46-17_MSK-PC-01/events.out.tfevents.1693305992.MSK-PC-01.740.0:  60%|#####9    | 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491dfeafdacd45ad85c2d86ef3c23f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Aug29_13-46-17_MSK-PC-01/events.out.tfevents.1693305992.MSK-PC-01.740.0:   4%|3         | 1.00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9c7daf47ac4573a3e2189467090bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file model.safetensors:   0%|          | 1.00k/922M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0063e36c54eb4fb3bd05a13b48beb4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/922M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b372d-4ec7-4a65-8dee-05470551b0af",
   "metadata": {
    "id": "ae3b372d-4ec7-4a65-8dee-05470551b0af"
   },
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea684fa6-4d80-4a9b-91e6-a7de094013ff",
   "metadata": {
    "id": "ea684fa6-4d80-4a9b-91e6-a7de094013ff"
   },
   "source": [
    "Обучение займет примерно 5-10 часов в зависимости от вашего GPU или того, какой GPU был выделен для этого Google Colab. Если вы используете Google Colab непосредственно для тонкой настройки модели Whisper, вы должны убедиться, что обучение не будет прервано из-за бездействия. \n",
    "Простым обходным решением для предотвращения этого является вставить следующий код в консоль этой вкладки (_нажать правую кнопку мыши_ -> _обзор_ -> вкладка _консоль_ -> _вставить код_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e942ed8-5fcb-43a8-9db6-e41afe954054",
   "metadata": {
    "id": "2e942ed8-5fcb-43a8-9db6-e41afe954054"
   },
   "source": [
    "```javascript\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\"); \n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "}\n",
    "setInterval(ConnectButton, 60000);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee981a-855a-412e-9132-fb30b2320b93",
   "metadata": {
    "id": "55ee981a-855a-412e-9132-fb30b2320b93"
   },
   "source": [
    "Пиковое потребление памяти GPU для данной конфигурации обучения составляет примерно 15,8 ГБ. \n",
    "В зависимости от GPU, выделенного в Google Colab, возможно, что при запуске обучения вы столкнетесь с ошибкой CUDA `\"out-of-memory\"`. \n",
    "В этом случае вы можете уменьшить `per_device_train_batch_size` постепенно в 2 раза и использовать [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps)\n",
    "для компенсации.\n",
    "\n",
    "Чтобы запустить обучение, просто выполните:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fc302ec-1bdc-4216-b8d6-e77d268bc286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 20282,
     "status": "error",
     "timestamp": 1673633973184,
     "user": {
      "displayName": "Артём Бойко",
      "userId": "08299330253633495858"
     },
     "user_tz": -180
    },
    "id": "6fc302ec-1bdc-4216-b8d6-e77d268bc286",
    "outputId": "b49f33f9-1b3e-42ce-a6af-913d3326b2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model:  artyomboyko/whisper-small-fine_tuned-ru\n",
      "Current dataset: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1501' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1501/10000 1:30:13 < 8:31:33, 0.28 it/s, Epoch 0.66/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.221861</td>\n",
       "      <td>17.939589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.222535</td>\n",
       "      <td>18.022201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1152' max='1274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1152/1274 20:00 < 02:07, 0.96 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent model: \u001b[39m\u001b[38;5;124m\"\u001b[39m, reference_model)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent dataset: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# trainer.train(resume_from_checkpoint=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# trainer.train(\"/home/artyom/Whisper_Train/whisper-small-fine_tuned-ru/checkpoint-3500\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2020\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2020\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2321\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2319\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2321\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:159\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m gen_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3053\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3050\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3052\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3053\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3063\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3245\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3242\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3244\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3245\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3246\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:276\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    274\u001b[0m ):\n\u001b[1;32m    275\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 276\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1663\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mreturn_timestamps:\n\u001b[1;32m   1661\u001b[0m     logits_processor \u001b[38;5;241m=\u001b[39m [WhisperTimeStampLogitsProcessor(generation_config)]\n\u001b[0;32m-> 1663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2339\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:581\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:569\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1419\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1416\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1417\u001b[0m         )\n\u001b[0;32m-> 1419\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1437\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1284\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1278\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1279\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1280\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1281\u001b[0m     )\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1284\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1099\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1089\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   1090\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m     )\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:555\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    546\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_attn(\n\u001b[1;32m    547\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    548\u001b[0m     key_value_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m--> 555\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresidual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# add cross-attn to positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    558\u001b[0m present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Current model: \", reference_model)\n",
    "print(\"Current dataset: \")\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train(\"/home/artyom/Whisper_Train/whisper-small-fine_tuned-ru/checkpoint-3500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db375909-5423-48e8-a7ef-ac6187754626",
   "metadata": {
    "id": "db375909-5423-48e8-a7ef-ac6187754626"
   },
   "source": [
    "Наш лучший WER составляет 32.0% - неплохо для 8 часов обучения! Мы можем отправить нашу контрольную точку в [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench) на push, задав соответствующие аргументы ключевых слов (kwargs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ea40887-4879-404f-9617-ba18ec01d535",
   "metadata": {
    "id": "0ea40887-4879-404f-9617-ba18ec01d535"
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: ru, split: test\",\n",
    "    \"language\": \"ru\",\n",
    "    \"model_name\": \"Whisper Small Ru - Artyom Boyko\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d84f1272-261b-43dd-9c9f-10d87ac47f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs = {\n",
    "#     \"dataset_tags\": dataset_name,\n",
    "#     \"dataset\": dataset_name.split(\"/\")[1],  # a 'pretty' name for the training dataset\n",
    "#     \"dataset_args\": \"config: ru, split: test\",\n",
    "#     \"language\": dataset_lang,\n",
    "#     # \"model_name\": \"Whisper Base Ru - Artyom Boyko\",  # a 'pretty' name for our model\n",
    "#     \"finetuned_from\": reference_model,\n",
    "#     \"tasks\": \"automatic-speech-recognition\",\n",
    "#     \"tags\": \"hf-asr-leaderboard\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eddae1-e0a4-4946-ae0f-a2451d6614e1",
   "metadata": {
    "id": "d2eddae1-e0a4-4946-ae0f-a2451d6614e1"
   },
   "source": [
    "Теперь результаты обучения можно загрузить в хаб. Для этого выполните команду `push_to_hub` и сохраните созданный нами объект препроцессора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc0e8645-cd8e-4045-9b77-c70c6947ef59",
   "metadata": {
    "id": "cc0e8645-cd8e-4045-9b77-c70c6947ef59"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b00ca-d6cf-4c11-b12c-b11176d520f3",
   "metadata": {
    "id": "EfUjFFnMQBQ9"
   },
   "source": [
    "## Загрузка выбранной контрольной точки в хаб (не обязательный шаг)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53b30c-54a3-4375-97fd-114005459990",
   "metadata": {},
   "source": [
    "Зададим путь в хабе для сохранения выбранной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3f990ad-e4ab-4e83-971d-4d7d16922f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_hub_path = \"artyomboyko/whisper-small-fine_tuned-ru\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae0d1d-ed41-4b85-b67e-6847b5cdea19",
   "metadata": {},
   "source": [
    "Загрузим в память выбранную по окончании процесса обучения модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9caaf15-a1be-4363-aef7-3ce5b2c4e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-small-fine_tuned-ru/checkpoint-3500/\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24a147-ba4e-4a08-81ce-559b18a189a8",
   "metadata": {},
   "source": [
    "Загруим в хаб модель из выбранной контрольной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c7ca1ef-7ca9-47fc-a74d-cdccf0077392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cc746a4d09410d8d7a813efa80fa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/artyomboyko/whisper-small-fine_tuned-ru/commit/fe677ff9eff3682bd06d2c39ed3cfa04be81ddfd', commit_message='Upload WhisperForConditionalGeneration', commit_description='', oid='fe677ff9eff3682bd06d2c39ed3cfa04be81ddfd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"artyomboyko/whisper-small-fine_tuned-ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038cc09-6fe5-48bf-ac5f-cf42ae4740ac",
   "metadata": {
    "id": "DSlt8M9kQBUa"
   },
   "source": [
    "> **ПРИМЕЧАНИЕ**:\n",
    "> ***Если загрузить токенизатор в хаб, может сломаться демо модели на странице модели в хабе.***    \n",
    "> ***НО! Если этого не сделать, то не сможет корректно работать демонстрация из-за невозможности загрузить токенизатор в конвеер.***\n",
    "> ***Решение этой проблемы - использование токенизатора исходной моедли \"openai/whisper-base\".***\n",
    "\n",
    "Сохраним токенизатор в хаб:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ogRPY1JVQBXg",
   "metadata": {
    "id": "ogRPY1JVQBXg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/artyomboyko/whisper-small-fine_tuned-ru/commit/6c4187ddf6a45fb4a6b34e9bb5830df4ab8a04a0', commit_message='Upload tokenizer', commit_description='', oid='6c4187ddf6a45fb4a6b34e9bb5830df4ab8a04a0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(hf_hub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f04e1-6f4a-4113-88a2-72efbdb584a9",
   "metadata": {},
   "source": [
    "Созраним в хаб препроцессор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "208ec816-ffce-4ab3-bfb2-5e5e1191c18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/artyomboyko/whisper-small-fine_tuned-ru/commit/072681218c584b63ef9254b038f73dd50302f5f8', commit_message='Upload processor', commit_description='', oid='072681218c584b63ef9254b038f73dd50302f5f8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.push_to_hub(hf_hub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827457b-5820-4300-aafe-63e77856ae8f",
   "metadata": {
    "id": "3827457b-5820-4300-aafe-63e77856ae8f"
   },
   "source": [
    "## Создание демонстрации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb6039-09af-4e54-930a-d1e2a7aacef5",
   "metadata": {},
   "source": [
    "Временно переключим режим логирования. Будем выводить только сообщения об ошибках и предупреждения. Это существенно сократит вывод в ячейках ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "caae1a8f-8670-4f49-8bdd-03c8927c5401",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c71828-8380-4643-bfba-a338cecf38fc",
   "metadata": {
    "id": "97c71828-8380-4643-bfba-a338cecf38fc"
   },
   "source": [
    "Теперь, когда мы доработали нашу модель, мы можем создать демонстрационный образец, чтобы продемонстрировать ее возможности ASR! \n",
    "Мы будем использовать конвейере 🤗 Transformers `pipeline`, который позаботится обо всем, начиная с предварительной обработки аудиовходов и заканчивая декодированием предсказаний модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32530aac-ba6e-455a-97f7-901ff4d92f93",
   "metadata": {
    "id": "32530aac-ba6e-455a-97f7-901ff4d92f93"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd66e5d212d470b8ca98ff252992317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/gradio/processing_utils.py:236: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "/home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"ElectricSecretAgent/whisper-base-fine_tuned-ru\", tokenizer=\"openai/whisper-base\")  # необходимо изменить на \"ваше_имя_пользователя/имя_модели\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Base - Artyom Boyko\",\n",
    "    description=\"Real-time demonstration for speech recognition in Russian using a finely tuned base Whisper model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39043577-1339-448a-a239-d59b1e50db51",
   "metadata": {},
   "source": [
    "Вернем режим логирования \"по умолчанию\" - выводим сообщения об ошибках, предупреждениях и основную информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a04b95e-ead8-4eac-b05e-857112594ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23aeaa-5870-4f33-9e95-64a661a456cd",
   "metadata": {
    "id": "ea23aeaa-5870-4f33-9e95-64a661a456cd"
   },
   "source": [
    "## Заключительные замечания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1abeeb3-863a-4400-913d-c19d95c6dc64",
   "metadata": {
    "id": "f1abeeb3-863a-4400-913d-c19d95c6dc64"
   },
   "source": [
    "В этом блоге мы рассмотрели пошаговое руководство по тонкой настройке Whisper для многоязыкового ASR используя 🤗 Datasets, Transformers и хаб Hugging Face. \n",
    "Для получения более подробной информации о модели Whisper, наборе данных Common Voice и теории, лежащей в основе тонкой настройки, обратитесь к сопроводительной записи [в блоге](https://huggingface.co/blog/fine-tune-whisper).\n",
    "Если вы заинтересованы в тонкой настройке других моделей Transformers, как для английского, так и для многоязычного ASR, обязательно ознакомьтесь с примерами скриптов в [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05700a-1d52-49b8-be91-8ae94e9ac4cb",
   "metadata": {
    "id": "2c05700a-1d52-49b8-be91-8ae94e9ac4cb"
   },
   "source": [
    "# Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc0302-ada3-488a-a3b5-670c6c5a0d34",
   "metadata": {
    "id": "56dc0302-ada3-488a-a3b5-670c6c5a0d34"
   },
   "source": [
    "- [Fine-Tune Whisper For Multilingual ASR with Transformers (Статья)](https://huggingface.co/blog/fine-tune-whisper)\n",
    "- [HuggingFace Logging](https://huggingface.co/docs/transformers/main_classes/logging)\n",
    "- [How to show the learning rate during training](https://discuss.huggingface.co/t/how-to-show-the-learning-rate-during-training/13914/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c41d61-57ea-4481-9eea-8ece55b6c94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c53984754d4dab9158434bbd37523c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23f5b55fead94386a3693c43547eb251",
      "placeholder": "​",
      "style": "IPY_MODEL_dbeda59754d747678b35d2f998379704",
      "value": "Downloading data: 100%"
     }
    },
    "01318a1365044fc49644200e3da8a7a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b476c9798ca428fb4188efbeb068164",
      "placeholder": "​",
      "style": "IPY_MODEL_6ba277e2abd94b4fb427ffb20c6c8bff",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "01e430b67361463d8c70a1943f6a06e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b53ab33375824d4690ac58d4c6ccf5c5",
      "max": 1967,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea3bbe1e18294d2882dcf889403cacfe",
      "value": 1967
     }
    },
    "01eab296e5e24f8f976b80d9cc9c5d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1257292285e44b49986fb03281e90d60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14517348c1544171804ab98ad6f25af8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "168764340c704fdf98cf343f9fbe3e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0c281f2a8a84e43b42d121604a3c4aa",
      "placeholder": "​",
      "style": "IPY_MODEL_254c0051d04c404fac8ba0ab4f4d0f8e",
      "value": " 1.97k/1.97k [00:00&lt;00:00, 133kB/s]"
     }
    },
    "1af9316db9204ec899077819ac4a33e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23f5b55fead94386a3693c43547eb251": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "254c0051d04c404fac8ba0ab4f4d0f8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b476c9798ca428fb4188efbeb068164": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3041aa5c6cca44f3ac7ed149a8f695da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "321707defc904da7a2baa6a0ffd454bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36c9544eb28b4a60b323fabbff3bb1f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_8b28ba2b19314571a51689b3767bb892",
      "style": "IPY_MODEL_7193255aba194762aca8ffd22d33f1d2",
      "tooltip": ""
     }
    },
    "3a77a46f4b22491581b4811abf73634d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4dd8b85931b476caf5fce8b899d9e5a",
       "IPY_MODEL_01e430b67361463d8c70a1943f6a06e2",
       "IPY_MODEL_168764340c704fdf98cf343f9fbe3e6d"
      ],
      "layout": "IPY_MODEL_ebe830bca86b47fcbc3a466a0f453b1d"
     }
    },
    "3d3761c108ea4cf8bf5a2e3c19fbb1ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_892fbc008e774549a4867629fa7a80ea",
      "style": "IPY_MODEL_56431bcba52f41cf82985766df22016e",
      "value": true
     }
    },
    "40d73b56aeab4ee79212f8a3f3b0223b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9424928a29a483d871ff5c9337e016f",
      "placeholder": "​",
      "style": "IPY_MODEL_5fa9c0392c034feda1b5e77c02e258ce",
      "value": " 588M/588M [00:35&lt;00:00, 17.8MB/s]"
     }
    },
    "45031fd2b0344c6ca326286e2f8b5315": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56431bcba52f41cf82985766df22016e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57d3dfb754a74770b50fa69bb7eb9e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ea0ba0d6c944625a918e26e5f7e44a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fa9c0392c034feda1b5e77c02e258ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "665a3db7544149ba955f1ea4de5552ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68a5f2e539cb4f08869923b400bdb40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6abd5d7f9c2248ed85d4b7f61dd11e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1af9316db9204ec899077819ac4a33e2",
      "placeholder": "​",
      "style": "IPY_MODEL_9489fa39837a4f05812064d4b0b1f83f",
      "value": " 4.49k/4.49k [00:00&lt;00:00, 249kB/s]"
     }
    },
    "6ba277e2abd94b4fb427ffb20c6c8bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d51bbda960144d78cba1585cbe4a644": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9edc322700ed4ce8bf234e28ced75617",
      "placeholder": "​",
      "style": "IPY_MODEL_b653be6fc7114033bcc2afd122f61302",
      "value": "Downloading builder script: 100%"
     }
    },
    "7193255aba194762aca8ffd22d33f1d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "79756643ded74db99294cb00275f2aa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aa6f6bc2aed4420907939a6fdb34446": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79756643ded74db99294cb00275f2aa7",
      "max": 4485,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae78207e9d5d439cab2a44e93e2ae85c",
      "value": 4485
     }
    },
    "7b8a33c170a544c6b16091753b3a5054": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_00c53984754d4dab9158434bbd37523c",
       "IPY_MODEL_da61ea52e72549db962c38b3f71cdea5",
       "IPY_MODEL_40d73b56aeab4ee79212f8a3f3b0223b"
      ],
      "layout": "IPY_MODEL_321707defc904da7a2baa6a0ffd454bd"
     }
    },
    "7ed67d56fce94d34b9393e396344809c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83fb9a9c5e0544d88d17dfa52669466a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1257292285e44b49986fb03281e90d60",
      "placeholder": "​",
      "style": "IPY_MODEL_fb5b3dfa404b472fac562aad2dc11b72",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "892fbc008e774549a4867629fa7a80ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b28ba2b19314571a51689b3767bb892": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93e2052e52d04220b5817189843da1a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9489fa39837a4f05812064d4b0b1f83f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9edc322700ed4ce8bf234e28ced75617": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0c281f2a8a84e43b42d121604a3c4aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2532cb8ce974839918aff4a99e9a28f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d51bbda960144d78cba1585cbe4a644",
       "IPY_MODEL_7aa6f6bc2aed4420907939a6fdb34446",
       "IPY_MODEL_6abd5d7f9c2248ed85d4b7f61dd11e6c"
      ],
      "layout": "IPY_MODEL_57d3dfb754a74770b50fa69bb7eb9e5b"
     }
    },
    "ae78207e9d5d439cab2a44e93e2ae85c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae9c664698a74e6ca277496fbbc96a9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af49c1f86fe24459b045603b4e924a5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2d47415bb924f6b927765131d9f0e12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f277dce872d64621883205fd1217345c",
      "placeholder": "​",
      "style": "IPY_MODEL_01eab296e5e24f8f976b80d9cc9c5d27",
      "value": " 967M/967M [00:18&lt;00:00, 40.7MB/s]"
     }
    },
    "b4dd8b85931b476caf5fce8b899d9e5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45031fd2b0344c6ca326286e2f8b5315",
      "placeholder": "​",
      "style": "IPY_MODEL_ae9c664698a74e6ca277496fbbc96a9d",
      "value": "Downloading: 100%"
     }
    },
    "b53ab33375824d4690ac58d4c6ccf5c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b653be6fc7114033bcc2afd122f61302": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caa97ce42d284b2aabc42a6cf53a6733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_665a3db7544149ba955f1ea4de5552ef",
      "placeholder": "​",
      "style": "IPY_MODEL_5ea0ba0d6c944625a918e26e5f7e44a4",
      "value": "Downloading: 100%"
     }
    },
    "ce189b2f200141abbba18fc4904e357b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_3041aa5c6cca44f3ac7ed149a8f695da",
      "placeholder": "​",
      "style": "IPY_MODEL_93e2052e52d04220b5817189843da1a1",
      "value": ""
     }
    },
    "da61ea52e72549db962c38b3f71cdea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14517348c1544171804ab98ad6f25af8",
      "max": 588154518,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68a5f2e539cb4f08869923b400bdb40a",
      "value": 588154518
     }
    },
    "dbeda59754d747678b35d2f998379704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc0675e545f6409997809fcb6cab6165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83fb9a9c5e0544d88d17dfa52669466a",
       "IPY_MODEL_ce189b2f200141abbba18fc4904e357b",
       "IPY_MODEL_3d3761c108ea4cf8bf5a2e3c19fbb1ff",
       "IPY_MODEL_36c9544eb28b4a60b323fabbff3bb1f9",
       "IPY_MODEL_01318a1365044fc49644200e3da8a7a8"
      ],
      "layout": "IPY_MODEL_dc0ceb8a76814014a97c26daa245a93c"
     }
    },
    "dc0ceb8a76814014a97c26daa245a93c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "e9424928a29a483d871ff5c9337e016f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea3bbe1e18294d2882dcf889403cacfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb53afde68d0440ba4c013e517c5f6ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f658de7c58c04e9bae9fb25702688d20",
      "max": 967092419,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af49c1f86fe24459b045603b4e924a5a",
      "value": 967092419
     }
    },
    "ebe830bca86b47fcbc3a466a0f453b1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f277dce872d64621883205fd1217345c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f658de7c58c04e9bae9fb25702688d20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f79a3e34697145c2829d0017fe14f62a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_caa97ce42d284b2aabc42a6cf53a6733",
       "IPY_MODEL_eb53afde68d0440ba4c013e517c5f6ea",
       "IPY_MODEL_b2d47415bb924f6b927765131d9f0e12"
      ],
      "layout": "IPY_MODEL_7ed67d56fce94d34b9393e396344809c"
     }
    },
    "fb5b3dfa404b472fac562aad2dc11b72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
