{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b1b3eb-dfbd-41cb-9656-26f992163aae",
   "metadata": {},
   "source": [
    "# Предварительная настройка среды разработки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7cfc2-ab19-4ebd-b39a-ceb906c5bbf3",
   "metadata": {},
   "source": [
    "## Глобальные параметры работы ноутбука"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05b20d-ecce-4884-8c70-da71d00cb32d",
   "metadata": {},
   "source": [
    "Глобальные параметры работы ноутбука для упрощения управления процессом обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9a6b180-bf66-4eca-aa1d-ec87d9df53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./DATA/\" # Относительный путь к папке для хранения данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbcdef-89b5-4def-a348-071a8ea29e44",
   "metadata": {},
   "source": [
    "## Загрузка вспомогательных модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adcc11d0-2c29-4e0f-8ea0-34183f580734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading auxiliary modules is completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print('Loading auxiliary modules is completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f3845-8f0b-4e2a-b0e1-c40d81a89c45",
   "metadata": {},
   "source": [
    "## Определение вспомогательных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f12df-1ef4-4d97-97a9-6a2598d41120",
   "metadata": {},
   "source": [
    "Определим некоторые вспомогательные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "400c0978-2089-440e-962f-416502624865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining auxiliary functions is complete.\n"
     ]
    }
   ],
   "source": [
    "# Функция для определения среды выполнения\n",
    "def is_google_colab():\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        print('Running on Google Colab')\n",
    "        return True\n",
    "    else:\n",
    "        print('Not running on Google CoLab')\n",
    "        return False\n",
    "\n",
    "\n",
    "# Функция-обёртка для выполнения команд оболочки\n",
    "def execute_shell_cmd(shell_cmd):\n",
    "  try:\n",
    "    result = subprocess.run(shell_cmd, stdout=subprocess.PIPE, shell=True)\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "  except Exception as e:\n",
    "    print(\"Expection message:\", e) \n",
    "    \n",
    "\n",
    "print('Defining auxiliary functions is complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccf719-b324-4eb2-998c-ffe530f87dd5",
   "metadata": {},
   "source": [
    "## Опредление и настройка среды выполнения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d958aa9-c2cc-4508-a744-9d968e282c64",
   "metadata": {},
   "source": [
    "Установим необходимые для данной среде выполнения бибилотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fd6e624-1cac-4d29-a415-38ce1b58d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google CoLab\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Requirement already satisfied: evaluate in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: xxhash in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.0.0)\n",
      "Requirement already satisfied: packaging in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (22.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: dill in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: filelock in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "\n",
      "Requirement already satisfied: jiwer in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from levenshtein==0.20.2->jiwer) (2.13.7)\n",
      "\n",
      "Requirement already satisfied: gradio in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: httpx in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.23.3)\n",
      "Requirement already satisfied: markdown-it-py[linkify,plugins] in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.1.0)\n",
      "Requirement already satisfied: pydub in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: uvicorn in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (4.4.0)\n",
      "Requirement already satisfied: pandas in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.5.2)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (10.4)\n",
      "Requirement already satisfied: pillow in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (9.3.0)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: pycryptodome in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.16.0)\n",
      "Requirement already satisfied: fsspec in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.8.3)\n",
      "Requirement already satisfied: pydantic in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.10.4)\n",
      "Requirement already satisfied: fastapi in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.89.1)\n",
      "Requirement already satisfied: requests in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.28.1)\n",
      "Requirement already satisfied: matplotlib in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.6.2)\n",
      "Requirement already satisfied: python-multipart in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: markupsafe in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: ffmpy in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: orjson in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.8.5)\n",
      "Requirement already satisfied: toolz in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: entrypoints in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->gradio) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: starlette==0.22.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from starlette==0.22.0->fastapi->gradio) (3.6.2)\n",
      "Requirement already satisfied: certifi in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (2022.12.7)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (0.16.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py~=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
      "Requirement already satisfied: mdit-py-plugins in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (1.0.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from python-multipart->gradio) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests->gradio) (1.26.13)\n",
      "Requirement already satisfied: click>=7.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from uvicorn->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from uvicorn->gradio) (0.14.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Команды для установки необходимых бибилиотек для Google Collab\n",
    "colab_libraries = [\"add-apt-repository -y ppa:jonathonf/ffmpeg-4\",\n",
    "                  \"apt update\",\n",
    "                  \"apt install -y ffmpeg\"]\n",
    "\n",
    "# Комадны для установки необходимых библиотек локальной среды\n",
    "local_libraries = ['conda install -y -c conda-forge transformers',\n",
    "                   'conda install -y -c conda-forge datasets', \n",
    "                   'conda install -y -c conda-forge librosa', \n",
    "                   'conda install -y -c conda-forge ffmpeg', \n",
    "                   'conda install -y -c conda-forge huggingface_hub', \n",
    "                   'conda install -y -c conda-forge tensorboard', \n",
    "                   'conda install -y -c conda-forge git-lfs', \n",
    "                   'pip install --no-input evaluate', \n",
    "                   'pip install --no-input jiwer', \n",
    "                   'pip install --no-input gradio']\n",
    "\n",
    "if is_google_colab():\n",
    "    for command in colab_libraries:\n",
    "        execute_shell_cmd(command)\n",
    "else:\n",
    "    for command in local_libraries:\n",
    "        execute_shell_cmd(command)\n",
    "\n",
    "print(\"Library installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a271842-36ee-4515-b30d-9f88c15abee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3d52e-78a3-4dda-8c79-0e0752c387f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa464d67-944b-4eda-8590-36a7d756ca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e9d1c-e5f1-4ec8-95d8-96ec397995c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd1071-f4f0-46e9-9ee5-3a3f08847a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0f01-dd3a-4bfb-9037-c54e8a0a0184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Google Collab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45f27e-f772-4937-b7dd-ba6837555684",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Подключение сервисов Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e5335-b715-4cf6-9681-c954c58f28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e26633-ae35-4b63-818c-d2ead300d88e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798cb81e-2303-4cba-bc69-30d3d059ccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b24bcf-fda3-4c4b-86ae-5979b36ef554",
   "metadata": {},
   "source": [
    "## Локальная среда"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8e550-e5d6-4476-96c9-a37e30016c33",
   "metadata": {},
   "source": [
    "### Проверка доступности GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed895ca-e0e7-478d-9020-f3cc7b7f937f",
   "metadata": {},
   "source": [
    "Чтобы получить GPU, нажмите _Runtime_ -> _Change runtime type_, затем измените _Hardware accelerator_ с _None_ на _GPU_.\n",
    "\n",
    "Мы можем проверить, что нам назначен GPU, и просмотреть его характеристики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8217a839-6751-4ae2-a1a4-9544a4496546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 12 17:46:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 527.92.01    Driver Version: 528.02       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P8     6W /  72W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A        32      G   /Xwayland                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc17294-0ccd-4844-a52d-5b09b0a5f4e3",
   "metadata": {},
   "source": [
    "### Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e881c0-5258-47d4-8119-16c237bc8a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: evaluate in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pandas in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (1.5.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.0.0)\n",
      "Requirement already satisfied: packaging in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (22.0)\n",
      "Requirement already satisfied: multiprocess in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: jiwer in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from levenshtein==0.20.2->jiwer) (2.13.7)\n",
      "Requirement already satisfied: gradio in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (3.16.1)\n",
      "Requirement already satisfied: pydantic in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.10.4)\n",
      "Requirement already satisfied: markupsafe in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: matplotlib in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.6.2)\n",
      "Requirement already satisfied: ffmpy in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: pydub in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: aiohttp in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.8.3)\n",
      "Requirement already satisfied: uvicorn in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: python-multipart in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: orjson in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.8.5)\n",
      "Requirement already satisfied: pillow in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (9.3.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (10.4)\n",
      "Requirement already satisfied: pyyaml in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: fastapi in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.89.1)\n",
      "Requirement already satisfied: markdown-it-py[linkify,plugins] in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.1.0)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2022.11.0)\n",
      "Requirement already satisfied: pandas in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (1.5.2)\n",
      "Requirement already satisfied: httpx in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (0.23.3)\n",
      "Requirement already satisfied: requests in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (2.28.1)\n",
      "Requirement already satisfied: pycryptodome in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from gradio) (3.16.0)\n",
      "Requirement already satisfied: toolz in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: entrypoints in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from pandas->gradio) (2022.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: starlette==0.22.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from starlette==0.22.0->fastapi->gradio) (3.6.2)\n",
      "Requirement already satisfied: certifi in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (2022.12.7)\n",
      "Requirement already satisfied: sniffio in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (0.16.3)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from httpx->gradio) (1.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py~=1.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
      "Requirement already satisfied: mdit-py-plugins in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (1.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (22.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from matplotlib->gradio) (4.38.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from python-multipart->gradio) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from requests->gradio) (1.26.13)\n",
      "Requirement already satisfied: click>=7.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from uvicorn->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from uvicorn->gradio) (0.14.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in /home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
      "Library installation complete!\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge transformers\n",
    "!conda install -y -c conda-forge datasets\n",
    "!conda install -y -c conda-forge librosa\n",
    "!conda install -y -c conda-forge ffmpeg\n",
    "!conda install -y -c conda-forge huggingface_hub\n",
    "!conda install -y -c conda-forge tensorboard            # Необходимо на этапе обучения с использованием Pytorch 1.13.1\n",
    "!conda install -y -c conda-forge git-lfs\n",
    "\n",
    "!pip install --no-input evaluate\n",
    "!pip install --no-input jiwer\n",
    "!pip install --no-input gradio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756687d-d234-4d16-990a-8d2000ef2652",
   "metadata": {},
   "source": [
    "### Загрузка необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6056b6-30b5-4d17-9f8d-2158e12b57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries/modules initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# Вскпомогательные бибилиотеки\n",
    "import os\n",
    "import multiprocessing\n",
    "import warnings\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from datasets import Audio\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "\n",
    "# Evaluate\n",
    "import evaluate\n",
    "\n",
    "print(\"Required libraries/modules initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecc953-8821-4064-97cc-dc17b5ccc74c",
   "metadata": {},
   "source": [
    "### Настройка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f54433-bfb3-48f9-9ce7-a980d4c0a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of workers processes is set at 16\n",
      "Environment setup completed!\n"
     ]
    }
   ],
   "source": [
    "# Определим количество ядер CPU, это влияет на количество рабочих процессов которые мы можем запустить одновременно\n",
    "system_num_workers = multiprocessing.cpu_count()\n",
    "print(\"The number of workers processes is set at\", system_num_workers)\n",
    "\n",
    "# Аутентификация ноутбука в HuggingFace HUB\n",
    "notebook_login()\n",
    "\n",
    "# Отключение назойлевых предупреждений\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "print(\"Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147ebd1-58e2-4ad7-a397-24e6805bd1a0",
   "metadata": {},
   "source": [
    "# Тонкая настройка Whisper для многоязыкового ASR с помощью библиотеки 🤗 Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca114a35-8e37-4e5d-bd57-165a43b8f79c",
   "metadata": {},
   "source": [
    "В этом блокноте мы представляем пошаговое руководство по тонкой настройке Whisper для любого многоязыкового набора данных ASR с помощью библиотеки Hugging Face 🤗 Transformers. Это более \"практическая\" версия сопутствующего [сообщения в блоге](https://huggingface.co/blog/fine-tune-whisper). Для более подробного объяснения Whisper, набора данных Common Voice и теории, лежащей в основе тонкой настройки, читателю рекомендуется обратиться к статье в блоге."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b0dcf-f121-407d-8688-ae7eed7bff9c",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5234d-999c-4ea6-aef6-a6a8f2fae199",
   "metadata": {},
   "source": [
    "Whisper - это предварительно обученная модель для автоматического распознавания речи (ASR) опубликованная в [сентябре 2022](https://openai.com/blog/whisper/) авторами Алеком Рэдфордом и другими из OpenAI. В отличие от многих своих предшественников, таких как. \n",
    "[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), которые предварительно обученны на неразмеченных аудиоданных, Whisper предварительно обучен на огромном количестве **размеченных** транскрибированных аудио данных, 680 000 часов, если быть точным. Это на порядок больше данных, чем использованные для обучения Wav2Vec 2.0 (60 000 часов) неразмеченных аудиоданных. Более того, 117 000 часов из этих данных использованных для предварительного обучения - это многоязыковые данные ASR. Это позволяет получить контрольные точки которые могут быть применены к более чем 96 языкам, многие из которых считаются _низкоресурсными_.\n",
    "\n",
    "При масштабировании на 680 000 часов размеченных данных предварительного обучения, модели Whisper демонстрируют высокую способность к обобщению для многих наборов данных и областей (доменов). Предварительно обученные контрольные точки достигают результатов, конкурентоспособных с современными \n",
    "ASR системами, с коэффициентом ошибок в словах (WER) около 3% на подмножестве чистых тестов LibriSpeech ASR и новым передовым результатом на TED-LIUM с 4,7% (смотрите таблицу 8 из [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)). \n",
    "\n",
    "Обширные знания о многоязычном ASR, полученные Whisper во время предварительного обучения могут быть использованы для других языков с низким уровнем ресурсов; посредством тонкой настройки предварительно обученные контрольные точки могут быть адаптированы для конкретных наборов данных и языков для дальнейшего улучшения результатов. Мы покажем, как можно точно настроить Whisper для языков с ограниченными ресурсами в этом блокноте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b4cc4-1ec3-4857-b11f-8aff83c0e06f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/whisper_architecture.svg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Рисунок 1:</b> модель Whisper. Архитектура соответствует стандартной модели кодера-декодера на основе трансформатора. Log-Mel спектрограмма подается на вход кодера. Последние скрытые состояния поступают в декодер через механизмы перекрестного внимания. Декодер авторегрессионно предсказывает текстовые токены, совместно обусловленные скрытыми состояниями энкодера и ранее предсказанными токенами. Источник рисунка: \n",
    "<a href=\"https://openai.com/blog/whisper/\">блог OpenAI Whisper</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731f8d5-ebd9-4a90-9564-a1b91ea5d890",
   "metadata": {},
   "source": [
    "Контрольные точки Whisper представлены в пяти конфигурациях с различными размерами моделей. Самые маленькие четыре модели обучаются либо только на английском, либо на многоязычных данных. Самая большая контрольная точка - только многоязычная. \n",
    "Все девять предварительно обученных контрольных точек доступны на сайте [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). Контрольные точки сведены в следующую таблицу со ссылками на модели в хабе:\n",
    "\n",
    "| Размер   | Количество слоёв | Ширина | Количество голов | Параметры | Только англоязычная модель                                         | Многоязычная модель                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "В демонстрационных целях мы доработаем контрольную точку многоязычной версии [``малой модели``](https://huggingface.co/openai/whisper-small) с 244M параметрами (~= 1GB). Что касается наших данных, мы обучим и оценим нашу систему на языке с низким уровнем ресурсов, взятом из набора данных [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Мы покажем, что всего за 8 часов тонкой настройки данных мы можем добиться высоких результатов на этом языке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d7e44-a8a4-4640-a07e-6ecb1b9f5bd0",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "${}^1$ - Название Whisper происходит от аббревиатуры \"WSPSR\", которая расшифровывается как \"Web-scale Supervised Pre-training for Speech Recognition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da24b6-3178-4538-8cd5-57ff8b22aaba",
   "metadata": {},
   "source": [
    "## Загрузка набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90357e66-1041-4c4f-9e7e-d40d94cae25b",
   "metadata": {},
   "source": [
    "Используя 🤗 Datasets, загрузка и подготовка данных чрезвычайно проста. \n",
    "\n",
    "Мы можем загрузить и подготовить сплиты Common Voice всего одной строкой кода. \n",
    "\n",
    "Во-первых, убедитесь, что вы приняли условия использования на Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). После принятия условий вы получите полный доступ к набору данных и сможете загружать данные локально.\n",
    "\n",
    "Объединим `train` и `validation` части набора данных, чтобы получить как можно больше данных для обучения. Будем использовать\n",
    "данных `test` в качестве нашего тестового набора.\n",
    "\n",
    "> ВАЖНО!\n",
    "> Для удобства хранения данных датасета, я ранее определил путь к папке в переменной `DATA_PATH`. Это позволит хранить кеш данных в известном заранее месте и в дальшнейшем удалить ненужные данные. Это позволит быстро освободить место на HDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa87700-7e44-457c-a593-38b9747263f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './DATA/' already exist!\n"
     ]
    }
   ],
   "source": [
    "# Создадим папку для хранения данных локально\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "    print(\"Directory '% s' created\" % DATA_PATH)\n",
    "else:\n",
    "    print(\"Directory '% s' already exist!\" % DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a82d17d-69bc-4e13-bdf1-4a058c367733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_2_0 (/home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079)\n",
      "Found cached dataset common_voice_2_0 (/home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 3898\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 1561\n",
      "    })\n",
      "})\n",
      "Data download successfully completed!\n"
     ]
    }
   ],
   "source": [
    "# Создадим словарь для хранения датасета\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_2_0\", \"ru\", split=\"train+validation\", cache_dir=DATA_PATH, use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_2_0\", \"ru\", split=\"test\", cache_dir=DATA_PATH, use_auth_token=True)\n",
    "\n",
    "print(common_voice)\n",
    "print(\"Data download successfully completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a920f22-3ede-492d-a7eb-9e764d3241c1",
   "metadata": {},
   "source": [
    "Большинство наборов данных ASR предоставляют только входные образцы аудио (`audio`) и соответствующий транскрибированный текст (`sentence`).  Common Voice содержит дополнительную метаданные, такие как `accent` и `locale`, которые мы можем игнорировать для ASR.\n",
    "\n",
    "Оставив блокнот максимально общим, мы рассматриваем только входной звук и транскрибированный текст для тонкой настройки, отбрасывая дополнительную информацию метаданных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79484dc-05a9-42b6-aed9-5289b0c60a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 3898\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 1561\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b572fa-b08d-4e94-8d20-6c341f23d1da",
   "metadata": {},
   "source": [
    "## Подготовка экстрактора признаков, токенизатора и данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e3a16-2c58-4e1c-808c-985adb82f22e",
   "metadata": {},
   "source": [
    "Конвейер ASR можно разделить на три этапа: \n",
    "1) экстрактор признаков, который предварительно обрабатывает необработанные аудиоданные\n",
    "2) Модель, которая выполняет сопоставление последовательности с последовательностью \n",
    "3) Токенизатор, который постобрабатывает выводы модели в текстовый формат.\n",
    "\n",
    "Модель Whisper из библиотеки 🤗 Transformers имеет ассоциированный экстрактор признаков и токенизатор, называемые [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
    "и [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer) \n",
    "соответственно.\n",
    "\n",
    "Рассмотрим детальнее настройки экстрактора и токенизатора по очереди!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e8c78-2986-4597-850d-5cbfcfdcdf78",
   "metadata": {},
   "source": [
    "### Загрузка WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da804fb-d0f5-49ae-8b4b-c370f135286a",
   "metadata": {},
   "source": [
    "Экстрактор признаков Whisper выполняет две операции:\n",
    "1. Разбивка / усечение аудиоданных до 30 секунд: все аудиоданные короче 30 секунд разбиваются на молчание (нули), а те, что длиннее 30 секунд, усекаются до 30 секунд.\n",
    "2. Преобразует входные аудио в _log-Mel спектрограммы_ входных признаков, визуальное представление аудио и форму входных данных, ожидаемых моделью Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c57886-db51-4867-86d8-33495e18d5b3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Рисунок 2:</b> Преобразование дискретизированного аудио массива в log-Mel спектрограмму.\n",
    "Слева: дискретизированный одномерный аудиосигнал. Справа: соответствующая log-Mel спектрограмма. Источник рисунка:\n",
    "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eabd2e-88c6-4617-8296-21f26e3dfc8e",
   "metadata": {},
   "source": [
    "Мы загрузим экстрактор признаков из предварительно обученной контрольной точки со параметрами по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a4b5d8-b946-48d0-b349-85db08a1968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature extractor is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "print(\"The feature extractor is loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf53f7-bec1-44ca-b1db-1aa395cef30f",
   "metadata": {},
   "source": [
    "### Загрузка WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d921f8-b3a4-4afc-8d67-35281063e47e",
   "metadata": {},
   "source": [
    "Модель Whisper выдает последовательность идентификаторов _токенов_. Токенизатор сопоставляет каждый из этих идентификаторов токенов с соответствующей текстовой строкой. Для русского языка мы можем загрузить предварительно обученный токенизатор и использовать его для тонкой настройки \n",
    "с указанием целевого языка и задачи. Эти аргументы сообщают токенизатору о том, что следует задавать префикс токена языка и задачи в начале последовательности меток:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3ad868-efde-495e-82aa-780f5c4b9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Russian\", task=\"transcribe\")\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434f3bf-5240-4650-9ca7-57edec2a59dd",
   "metadata": {},
   "source": [
    "### Комбинирование для создания WhisperProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d28df3-e0de-499d-8141-991d1785282f",
   "metadata": {},
   "source": [
    "Чтобы упростить использование экстрактора признаков и токенизатора, мы можем _обернуть_ их в один класс `WhisperProcessor`. Этот объект наследуется от `WhisperFeatureExtractor` и `WhisperProcessor`, и может использоваться для обработки входных аудиоданных и \n",
    "получения предсказаний модели по мере необходимости. При этом во время обучения нам нужно отслеживать только два объекта: `processor` и `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef4a58f-393e-4a6f-8813-fa3d7904b961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Russian\", task=\"transcribe\")\n",
    "\n",
    "print(\"Processor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a495cd5-cd68-4cb5-934b-4a0d885c38c1",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfe667-ebeb-43cf-8bed-14a666eb7dec",
   "metadata": {},
   "source": [
    "Давайте распечатаем первый образец набора данных Common Voice, чтобы посмотреть в какой форме находятся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80eb74a1-99c2-4165-a151-f03a9b2b044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'DATA/downloads/extracted/acaf2c2e872a1554fdde5f2e9f14ba8c2f8793da2206b75dafc9b15a35bc343a/clips/001b11fa8d4b0277e41ab82671346ec1b8851e6ed7162c12952a45c9ffc3d8a61b333e92188ebfc6d61b128e1967fb0074b4d22208dc32f41a4f4a23e6ec1f00.mp3', 'array': array([ 0.0000000e+00,  4.5755869e-13,  8.2663303e-13, ...,\n",
      "       -1.3604904e-07, -2.0340853e-05, -7.7015411e-06], dtype=float32), 'sampling_rate': 48000}, 'sentence': 'Да ты уже просто с катушек съехал!'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250c661-248c-48f0-bb0c-0097d6455c73",
   "metadata": {},
   "source": [
    "Поскольку наш входной звук сэмплирован с частотой 48 кГц (`'...sampling_rate': 48000...`), нам нужно _уменьшить_ его до частоту дискретизации до 16 кГц перед тем, как передать его в экстрактор признаков Whisper, 16 кГц - это частота дискретизации, ожидаемая моделью Whisper. Мы установим для аудио входов правильную частоту дискретизации с помощью метода [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column). Эта операция не изменяет звук \"на месте\", а скорее дает сигнализирует `datasets` передискретизировать аудиосэмплы _на лету_ при при первой загрузке данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59a5789-c125-4371-96c5-81f7d345774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample rate set successfully.\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(\"Audio sample rate set successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691d753-3d67-4724-bc08-d8018ba66795",
   "metadata": {},
   "source": [
    "Повторная загрузка первого аудиообразца из набора данных Common Voice приведет к его передискретизации до нужной частоты дискретизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e51d92-dd89-4690-ae1d-aa9cc358163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'DATA/downloads/extracted/acaf2c2e872a1554fdde5f2e9f14ba8c2f8793da2206b75dafc9b15a35bc343a/clips/001b11fa8d4b0277e41ab82671346ec1b8851e6ed7162c12952a45c9ffc3d8a61b333e92188ebfc6d61b128e1967fb0074b4d22208dc32f41a4f4a23e6ec1f00.mp3', 'array': array([ 3.0140032e-13,  7.2592689e-15, -1.1101574e-12, ...,\n",
      "       -1.6450653e-05,  2.4153996e-07, -1.7190577e-06], dtype=float32), 'sampling_rate': 16000}, 'sentence': 'Да ты уже просто с катушек съехал!'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36b2eb-851f-4f6f-9222-18d24a18de87",
   "metadata": {},
   "source": [
    "Теперь мы можем написать функцию для подготовки наших данных к работе с моделью:\n",
    "1. Мы загружаем и передискретизируем аудиоданные, вызывая `batch[\"audio\"]`. Как объяснялось выше, 🤗 Datasets выполняет все необходимые операции по передискретизации на лету.\n",
    "2. Мы используем экстрактор признаков для вычисления входных признаков спектрограммы log-Mel из нашего одномерного аудио массива.\n",
    "3. Мы кодируем транскрипции в идентификаторы меток с помощью токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51abe62-7257-4363-8e15-6abfbe65cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Загрузим и передискретизировать аудиоданные с 48 до 16 кГц\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # вычислим log-Mel входные признаки из входного аудио массива  \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # закодируем целевой текст в идентификаторы меток\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0f12d-1dc0-4b09-9a0e-36883e91231e",
   "metadata": {},
   "source": [
    "Мы можем применить функцию подготовки данных ко всем нашим учебным примерам, используя метод `.map` в наборе данных. Аргумент `num_proc` указывает, сколько ядер процессора использовать. Если задать `num_proc` > 1, то будет включена многопроцессорная обработка. Если метод `.map` зависает при многопроцессорной обработке, установите `num_proc=1` и обрабатывайте набор данных последовательно.\n",
    "\n",
    "> ВАЖНО!\n",
    "> Для удобства работы я использую автоматическое определения количества рабочих прощессов через переменную `system_num_workers`. Это позволяет существенно повысить производительность и сэкономить время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "911c267f-d85d-4e47-8f40-549ce904ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number of CPU cores is 16.\n",
      "                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-ad6f6728803ba2e0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-4f996e486d542d74.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-cd8007f8f33fbae9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-a8d5dbf5e9569c04.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-dae49921e5d6541f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-b22da070f351f480.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-129ed9c5e905dcd9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-41cc4a063736a1b2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-313eb7b083917740.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-b5be9bb9a20709e5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-0c048e80858ad146.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-2e5869bd913b0e1d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-01ef408a7c8fa8a8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-99af187654597390.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-a50980cb3361f929.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-11e3e5f36b129dbd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-abb0e6867adbbb1d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-83e8ba4c60c1386b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-538e2a915fc2f1d9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-fb0074d8262696d1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-a13b6851c1dd2ab8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-cb0a4d1dfa0ff00a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-b520e04405c5fd99.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-75a1cca7d948c5b1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-f32604c2687d7ef3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-1c44938b464a22fe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-abf247cdbb570a87.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-5985d429851b01b6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-3f38e2efeffefc92.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-d26fcc1a22c3b2e7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-b45312058cda5192.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/artyom/Whisper_Train/./DATA/mozilla-foundation___common_voice_2_0/ru/2.0.0/c093ea99a0b8cadf95dd03e2ba81d28119744728ae97cc6196fd0c17b9b7f079/cache-c287248cbe9ff34d.arrow\n"
     ]
    }
   ],
   "source": [
    "print(\"The current number of CPU cores is {}.\".format(system_num_workers))\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=system_num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b781aaa-1077-42d8-9a77-5d634db60f88",
   "metadata": {},
   "source": [
    "## Обучение и оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1beb657-2884-480c-9a1b-5529d039b6c1",
   "metadata": {},
   "source": [
    "Теперь, когда мы подготовили наши данные, мы готовы погрузиться в конвейер обучения. [🤗Тренер (Trainer)](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) сделает за нас большую часть тяжелой работы. Все, что нам нужно сделать, это:\n",
    "- Определить коллатор данных (data collator): коллатор данных берет наши предварительно обработанные данные и подготавливает тензоры PyTorch, готовые для модели.\n",
    "- Выбрать метрику для оценки: во время оценки мы хотим оценить модель с помощью метрики [word error rate (WER)](https://huggingface.co/metrics/wer). Нам нужно определить функцию `compute_metrics`, которая будет обрабатывать эти вычисления.\n",
    "- Загрузить предварительно обученную контрольную точку: нам нужно загрузить предварительно обученную контрольную точку и правильно настроить ее для обучения.\n",
    "- Определить конфигурацию процесса обучения: она будет использоваться 🤗 тренером для определения расписания тренировок.\n",
    "\n",
    "После того как мы произведем тонкую настройку модели, мы оценим ее на тестовых данных, чтобы убедиться, что мы правильно обучили ее транскрибировать речь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5882122-798a-4cd2-b0f8-ec5c290c49ea",
   "metadata": {},
   "source": [
    "### Определение коллатора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78853c4-bae6-46c9-8536-ae7bc8663f5f",
   "metadata": {},
   "source": [
    "Коллатор данных для модели последовательной речи уникален в том смысле, что он обрабатывает входные признаки `input_features` и метки `labels` независимо друг от друга: входные признаки `input_features` должны быть обработанны экстрактором признаков, а метки `labels` - токенизатором. \n",
    "\n",
    "Входные признаки `input_features` уже разбиты на аудиофрагменты длительностью 30 секунд и преобразованы в соответствующие им спектрограммы log-Mel фиксированной размерности с помощью экстрактора признаков, поэтому все, что нам нужно сделать, это преобразовать входные признаки `input_features` в тензоры PyTorch. Мы делаем это с помощью метода `.pad` экстрактора признаков с хаданием параметра `return_tensors=pt`.\n",
    "\n",
    "С другой стороны, `метки (labels)` не имеют вставок (un-padded). Сначала мы выравниваем последовательности до максимальной длины в батче, используя метод `.pad` токенизатора. Затем токены-вставки заменяются на значение `-100`, чтобы эти токены **не** учитывались при вычислении потерь. Затем мы вырезаем токен BOS из начала последовательности меток, так как мы добавим его позже во время обучения. \n",
    "\n",
    "Мы можем использовать препроцессор `WhisperProcessor`, который мы определили ранее, для выполнения извлечения признаков, так и для работы с токенами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9a116b-8b60-451b-b274-f177c6adcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c9591-cda5-47fb-8438-7d0d1859fc08",
   "metadata": {},
   "source": [
    "Давайте инициализируем коллатор данных, который мы только что определили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682bd554-b814-457e-8239-2d9db732c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f1ac2-419e-4e34-b6c8-96dee3ede7a8",
   "metadata": {},
   "source": [
    "### Метрика оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689a5a1-64ba-4fe9-aac4-f2c7859f2ed7",
   "metadata": {},
   "source": [
    "Мы будем использовать метрику коэффициента ошибок в словах (WER), которая является \"де-факто\" метрикой для оценки систем ASR-систем. За дополнительной информацией обратитесь к документу WER [docs](https://huggingface.co/metrics/wer). Мы загрузим метрику WER из 🤗 Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4750e8-237f-4dd7-bd2d-27004238c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "print(\"Metric is loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f2f95-0572-4f69-ac2c-b50185116464",
   "metadata": {},
   "source": [
    "Затем нам просто нужно определить функцию, которая принимает предсказания нашей модели и возвращает метрику WER. Эта функция, называемая `compute_metrics`, сначала заменяет `-100` на `pad_token_id` в `label_ids` (отменяя шаг, который мы применили в \n",
    "в коллаторе данных, чтобы правильно игнорировать подстановочные токены в потерях). Затем она декодирует прогнозы и идентификаторы меток в строки. Наконец, она вычисляет WER между прогнозами (predictions) и эталонными метками (reference labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "051b01b4-a27b-48bd-a0c2-dcd5cdb8da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compute_metrics function was defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # заменяем -100 на pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # мы не хотим группировать токены при вычислении метрики\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"The compute_metrics function was defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8caa9-e1bd-428a-8672-d4ac3f5d4387",
   "metadata": {},
   "source": [
    "### Загрузка контрольной точки предварительно обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21beb9-e548-413e-8e0e-50c80e83372f",
   "metadata": {},
   "source": [
    "Теперь давайте загрузим предварительно обученную контрольную точку модели Whisper `small`. Опять же, это тривиально благодаря использованию библиотеки 🤗 Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6eee780-8dbd-432e-9bf5-3d5eb8a713df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint of the pre-trained model has been successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "print(\"Checkpoint of the pre-trained model has been successfully loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127c743-15cc-4410-ab15-ab0a245a0fe3",
   "metadata": {},
   "source": [
    "Переопределение аргументов генерации - никакие токены не выдаются принудительно в качестве выходов декодера (смотрите [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), никакие токены не подавляются во время генерации (дополнительно смотрите [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbf3e2d9-8a50-4e72-8997-083d18dd3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374310ce-c37c-4656-a0aa-f4437dd830c0",
   "metadata": {},
   "source": [
    "### Определение конфигурации обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cec3af-d2ea-4eda-b5b5-339d850d1c5e",
   "metadata": {},
   "source": [
    "На последнем этапе мы определяем все параметры, связанные с процессом обучения. Более подробную информацию об аргументах процесса обучения можно найти в разделе Seq2SeqTrainingArguments [документации](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d081f13c-21b6-4ff5-bb66-7b6e873a52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi-ru\",  # измените имя репозитория по своему усмотрению\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,  # увеличивается в 2x раза при каждом уменьшении размера батча в 2x раза  \n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c269c7-397c-47a6-ba8b-eadc600a262c",
   "metadata": {},
   "source": [
    "> **Примечание**: если вы не хотите загружать контрольные точки модели в хаб HunggingFace, установите параметр `push_to_hub=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4d169-8852-44c5-85b9-d81e43414e60",
   "metadata": {},
   "source": [
    "Теперь мы можем передать аргументы обучения в 🤗 Trainer вместе с нашей моделью, набором данных, коллатором данных и функцией `compute_metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d7c4326-235c-4a53-806d-4d1b1d0a48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artyom/Whisper_Train/./whisper-small-hi-ru is already a clone of https://huggingface.co/ElectricSecretAgent/whisper-small-hi-ru. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b372d-4ec7-4a65-8dee-05470551b0af",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea684fa6-4d80-4a9b-91e6-a7de094013ff",
   "metadata": {},
   "source": [
    "Обучение займет примерно 5-10 часов в зависимости от вашего GPU или того, какой GPU был выделен для этого Google Colab. Если вы используете Google Colab непосредственно для тонкой настройки модели Whisper, вы должны убедиться, что обучение не будет прервано из-за бездействия. \n",
    "Простым обходным решением для предотвращения этого является вставить следующий код в консоль этой вкладки (_нажать правую кнопку мыши_ -> _обзор_ -> вкладка _консоль_ -> _вставить код_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e942ed8-5fcb-43a8-9db6-e41afe954054",
   "metadata": {},
   "source": [
    "```javascript\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\"); \n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "}\n",
    "setInterval(ConnectButton, 60000);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee981a-855a-412e-9132-fb30b2320b93",
   "metadata": {},
   "source": [
    "Пиковая память GPU для данной конфигурации обучения составляет примерно 15,8 ГБ. \n",
    "В зависимости от GPU, выделенного в Google Colab, возможно, что при запуске обучения вы столкнетесь с ошибкой CUDA `\"out-of-memory\"`. \n",
    "В этом случае вы можете уменьшить `per_device_train_batch_size` постепенно в 2 раза и использовать [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps)\n",
    "для компенсации.\n",
    "\n",
    "Чтобы запустить обучение, просто выполните:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fc302ec-1bdc-4216-b8d6-e77d268bc286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artyom/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3898\n",
      "  Num Epochs = 17\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 241734912\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/4000 00:14 < 8:02:54, 0.14 it/s, Epoch 0.01/17]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.21 GiB already allocated; 0 bytes free; 3.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/transformers/trainer.py:1811\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m   1810\u001b[0m     scale_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[0;32m-> 1811\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m   1813\u001b[0m     scale_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:341\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 341\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:288\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 288\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-1.13.1/lib/python3.10/site-packages/transformers/optimization.py:362\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    360\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 362\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    364\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.21 GiB already allocated; 0 bytes free; 3.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db375909-5423-48e8-a7ef-ac6187754626",
   "metadata": {},
   "source": [
    "Наш лучший WER составляет 32.0% - неплохо для 8 часов обучения! Мы можем отправить нашу контрольную точку в [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench) на push, задав соответствующие аргументы ключевых слов (kwargs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea40887-4879-404f-9617-ba18ec01d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"hi\",\n",
    "    \"model_name\": \"Whisper Small Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eddae1-e0a4-4946-ae0f-a2451d6614e1",
   "metadata": {},
   "source": [
    "Теперь результаты обучения можно загрузить в хаб. Для этого выполните команду `push_to_hub` и сохраните созданный нами объект препроцессора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e8645-cd8e-4045-9b77-c70c6947ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827457b-5820-4300-aafe-63e77856ae8f",
   "metadata": {},
   "source": [
    "## Создание демонстрации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c71828-8380-4643-bfba-a338cecf38fc",
   "metadata": {},
   "source": [
    "Теперь, когда мы доработали нашу модель, мы можем создать демонстрационный образец, чтобы продемонстрировать ее возможности ASR! \n",
    "Мы будем использовать конвейере 🤗 Transformers `pipeline`, который позаботится обо всем, начиная с предварительной обработки аудиовходов и заканчивая декодированием предсказаний модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32530aac-ba6e-455a-97f7-901ff4d92f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"sanchit-gandhi/whisper-small-hi\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23aeaa-5870-4f33-9e95-64a661a456cd",
   "metadata": {},
   "source": [
    "## Заключительные замечания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1abeeb3-863a-4400-913d-c19d95c6dc64",
   "metadata": {},
   "source": [
    "В этом блоге мы рассмотрели пошаговое руководство по тонкой настройке Whisper для многоязыкового ASR используя 🤗 Datasets, Transformers и хаб Hugging Face. \n",
    "Для получения более подробной информации о модели Whisper, наборе данных Common Voice и теории, лежащей в основе тонкой настройки, обратитесь к сопроводительной записи [blog post](https://huggingface.co/blog/fine-tune-whisper).\n",
    "Если вы заинтересованы в тонкой настройке других моделей Transformers, как для английского, так и для многоязычного ASR, обязательно ознакомьтесь с примерами скриптов в [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05700a-1d52-49b8-be91-8ae94e9ac4cb",
   "metadata": {},
   "source": [
    "# Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc0302-ada3-488a-a3b5-670c6c5a0d34",
   "metadata": {},
   "source": [
    "- [Fine-Tune Whisper For Multilingual ASR with Transformers (Статья)](https://huggingface.co/blog/fine-tune-whisper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
